import os
from dotenv import load_dotenv
from typing import List, Dict
import re
from pathlib import Path

load_dotenv("apikey.env")  # .env ë¡œë“œ
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["PINECONE_API_KEY"] = os.getenv("PINECONE_API_KEY")

# ì´í›„ ì½”ë“œ: PDF ë¡œë“œ, ì²­í¬ë§, ì„ë² ë”© ë“± ì¶”ê°€ ì˜ˆì •
print("í™˜ê²½ ì„¤ì • ì™„ë£Œ!")

from llama_index.core import SimpleDirectoryReader

# í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ê³„ì‚°
current_dir = Path(__file__).parent  # /Users/kimdaegi/Desktop/backend/app/loding
project_root = current_dir.parent    # /Users/kimdaegi/Desktop/backend/app
pdfs_dir = project_root / "pdfs"     # /Users/kimdaegi/Desktop/backend/app/pdfs

reader = SimpleDirectoryReader(input_dir=str(pdfs_dir))
documents = reader.load_data()
print(f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")

# ê° ë¬¸ì„œë³„ ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
'''
print("\n=== ê° ë¬¸ì„œë³„ ìƒì„¸ ì •ë³´ ===")
for i, doc in enumerate(documents):
    # íŒŒì¼ëª… ì¶”ì¶œ (metadataì—ì„œ)
    filename = doc.metadata.get('file_name', f'ë¬¸ì„œ_{i+1}')
    file_path = doc.metadata.get('file_path', 'Unknown')
    
    print(f"\nğŸ“„ ë¬¸ì„œ {i+1}: {filename}")
    print(f"   íŒŒì¼ ê²½ë¡œ: {file_path}")
    print(f"   í…ìŠ¤íŠ¸ ê¸¸ì´: {len(doc.text)} ë¬¸ì")
    
    # ì²« 200ì ë¯¸ë¦¬ë³´ê¸°
    preview_text = doc.text[:200].replace('\n', ' ').strip()
    print(f"   ì²« 200ì ë¯¸ë¦¬ë³´ê¸°: {preview_text}...")
    
    # ë©”íƒ€ë°ì´í„° ì •ë³´
    print(f"   ë©”íƒ€ë°ì´í„°: {doc.metadata}")
    print("-" * 80)

print(f"\nì´ ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜: {len(documents)}ê°œ")
'''
# Detect ì¡°ë¬¸ ì œëª© ë° ë³¸ë¬¸ ì²­í¬

def extract_chunks_finditer(text: str, filename: str) -> List[Dict]:
    # ì¡°í•­ ì œëª© íŒ¨í„´ íƒìƒ‰
    pattern = re.compile(r"(ì œ\s*\d+ì¡°(?:ì˜\d+)?)(\([^)]{1,30}\))")  
    matches = list(pattern.finditer(text))

    print(f"ğŸ” {filename}: ì´ {len(matches)}ê°œ ì¡°í•­ ì œëª© ë°œê²¬")

    chunks = []
    for i in range(len(matches)):
        start = matches[i].start()  # í˜„ì¬ ì¡°í•­ ì‹œì‘
        end = matches[i+1].start() if i + 1 < len(matches) else len(text)  # ë‹¤ìŒ ì¡°í•­ ì „ê¹Œì§€
        chunk_text = text[start:end].strip()

        law_id = matches[i].group(1).replace(" ", "")
        title = matches[i].group(2).strip("()")
        ref_stars = detect_star_references(chunk_text)
        
        chunks.append({
            "text": chunk_text,
            "metadata": {
                "law_article_id": law_id,        # ì¡°ë¬¸ë²ˆí˜¸ â†’ law_article_id
                "title": title,                  # ì œëª© (ì˜ë¬¸ ìœ ì§€)
                "source_file": filename,         # ì†ŒìŠ¤íŒŒì¼ (ì˜ë¬¸ ìœ ì§€)
                "category": "law_articles",      # ì¹´í…Œê³ ë¦¬ (ì˜ë¬¸ ìœ ì§€)
                "referenced_tables": ref_stars   # ì°¸ì¡°ë³„í‘œ â†’ referenced_tables
            }
        })
        
        # ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥
        print(f"\nğŸ§© ì²­í¬ {i+1}")
        print(f"ì¡°ë¬¸ë²ˆí˜¸: {law_id}")
        print(f"ì œëª©: {title}")
        print(f"ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {chunk_text[:100].replace('\n', ' ')}...")
        print(f"ì°¸ì¡° ë³„í‘œ: {', '.join(ref_stars) if ref_stars else 'ì—†ìŒ'}")
        print("-" * 40)

    print(f"\nâœ… {filename} â†’ {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
    return chunks

# Detect ë³„í‘œ ë¸”ë¡
def extract_star_tables(text: str, filename: str, law_blocks: List[Dict]) -> List[Dict]:
    pattern = re.compile(r"\<ë³„í‘œ\s*\d+\>")
    matches = list(pattern.finditer(text))

    print(f"\nğŸ“Œ {filename} - ë³„í‘œ ë¸”ë¡ {len(matches)}ê°œ ë°œê²¬ë¨")

    star_chunks = []
    for i, match in enumerate(matches):
        start = match.start()
        end = matches[i+1].start() if i+1 < len(matches) else len(text)
        star_text = text[start:end].strip()

        star_id = match.group().strip("<>").replace(" ", "")

        # ì—°ê²° ì¡°ë¬¸ ì¶”ì •: ê°€ì¥ ê°€ê¹Œìš´ ì•ì„  ì¡°ë¬¸ ë¸”ë¡ ì°¾ê¸°
        parent_law = None
        for law in reversed(law_blocks):
            if (law['metadata']['source_file'] == filename and 
                law['metadata']['law_article_id']):  # í•„ë“œëª… ë³€ê²½
                if law['text'] and law['text'].find(match.group()) > -1:
                    parent_law = law['metadata']['law_article_id']  # í•„ë“œëª… ë³€ê²½
                    break

        star_chunks.append({
            "text": star_text,
            "metadata": {
                "table_id": star_id,              # ë³„í‘œë²ˆí˜¸ â†’ table_id
                "category": "appendix_tables",    # ì¹´í…Œê³ ë¦¬ ì˜ë¬¸í™”
                "parent_law_article": parent_law or "unspecified",  # parent_ì¡°ë¬¸ â†’ parent_law_article
                "source_file": filename
            }
        })

        print(f"ğŸ§© ë³„í‘œ ì²­í¬ {i+1}: {star_id} (ì—°ê²°: {parent_law})")
        print(f"   ë¯¸ë¦¬ë³´ê¸°: {star_text[:80].replace('\n', ' ')}...")

    return star_chunks
# Detect ë³„í‘œ ì°¸ì¡°
def detect_star_references(text: str) -> List[str]:
    pattern = re.compile(r"\<ë³„í‘œ\s*\d+\>")
    return [
        m.group().strip("<>").replace(" ", "")  # ì˜ˆ: 'ë³„í‘œ1'
        for m in pattern.finditer(text)
    ]

# -------------------------------
# ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬ ë° ìš”ì•½
# -------------------------------
all_chunks: List[Dict] = []
for i, doc in enumerate(documents):
    fname = doc.metadata.get('file_name', f'ë¬¸ì„œ_{i+1}')
    chunks = extract_chunks_finditer(doc.text, fname)
    all_chunks.extend(chunks)
    star_chunks = extract_star_tables(doc.text, fname, chunks)
    all_chunks.extend(star_chunks)



