import math
import os
import json
import uuid
import asyncio
from typing import Any, Dict, List, Optional, AsyncGenerator
from datetime import datetime

# ìƒˆë¡œìš´ import ê²½ë¡œ
from app.ai.chatbot.config import model, client, currTime
from app.ai.chatbot.metadata import FunctionCallMetadata, RagMetadata, ChatMetadata, TokenUsageMetadata, ToolReasoningMetadata
from app.ai.functions import FunctionCalling, tools
from app.ai.rag.service import RagService
from app.ai.utils.token_counter import TokenCounter
from app.ai.utils.cost_calculator import CostCalculator
# from app.ai.events.chat_observer import chat_observer, ChatEvent  # í˜‘ì˜ í›„ í™œì„±í™” ì˜ˆì •

# LLM Manager import
from app.ai.llm import get_provider, get_llm_manager

class ChatbotStream:
    def __init__(self, model,system_role,instruction,**kwargs):
        """
        ì´ˆê¸°í™”:
          - context ë¦¬ìŠ¤íŠ¸ ìƒì„± ë° ì‹œìŠ¤í…œ ì—­í•  ì„¤ì •
          - sub_contexts ì„œë¸Œ ëŒ€í™”ë°© ë¬¸ë§¥ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ {í•„ë“œì´ë¦„,ë¬¸ë§¥,ìš”ì•½,ì§ˆë¬¸} êµ¬ì„±
          - current_field = í˜„ì¬ ëŒ€í™”ë°© ì¶”ì  (ê¸°ë³¸ê°’: ë©”ì¸ ëŒ€í™”ë°©
          - openai.api_key ì„¤ì •
          - ì‚¬ìš©í•  ëª¨ë¸ëª… ì €ì¥
          - ì‚¬ìš©ì ì´ë¦„
        """
        # í˜„ì¬ ë‚ ì§œë¥¼ system_roleì— ì¶”ê°€
        current_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
        system_role_with_date = f"{system_role}\n\ní˜„ì¬ ë‚ ì§œ: {current_date}"

        self.system_role = system_role_with_date
        self.context = [{"role": "system","content": system_role_with_date}]
               
        self.current_field = "main"
        
        self.model = model
        self.instruction=instruction

        self.max_token_size = 16 * 1024 #ìµœëŒ€ í† í°ì´ìƒì„ ì“°ë©´ ì˜¤ë¥˜ê°€ë°œìƒ ë”°ë¼ì„œ í† í° ìš©ëŸ‰ê´€ë¦¬ê°€ í•„ìš”.
        self.available_token_rate = 0.9#ìµœëŒ€í† í°ì˜ 90%ë§Œ ì“°ê² ë‹¤.

        # ë””ë²„ê·¸ í”Œë˜ê·¸ (í™˜ê²½ë³€ìˆ˜ RAG_DEBUGë¡œ ì œì–´: ê¸°ë³¸ í™œì„±í™”)
        self.debug = os.getenv("RAG_DEBUG", "1") not in ("0", "false", "False")

        # Phase 3: í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ì¶”ì  (ë¨¼ì € ì´ˆê¸°í™”)
        self.token_counter = TokenCounter(model=model)
        self.cost_calculator = CostCalculator()

        # Phase 2: ëª¨ë“ˆí˜• RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤í™” (token_counter ì „ë‹¬)
        self.rag_service = RagService(debug_fn=self._dbg, token_counter=self.token_counter)
        self._last_rag_result = None
        self._last_web_status: Optional[str] = None
        
        # Phase 2: í•¨ìˆ˜ í˜¸ì¶œ ê´€ë ¨ ì¸ìŠ¤í„´ìŠ¤í™” (token_counter ì „ë‹¬)
        self.func_calling = FunctionCalling(model=model, token_counter=self.token_counter)
        self.tools = tools
        self.available_functions = self.func_calling.available_functions if hasattr(self.func_calling, 'available_functions') else {}
        
        # Phase 2.5: async í•¨ìˆ˜ íƒ€ì… ë¯¸ë¦¬ ì²´í¬í•˜ì—¬ ìºì‹± (ì„±ëŠ¥ ìµœì í™”)
        self._async_function_flags = {
            name: asyncio.iscoroutinefunction(func)
            for name, func in self.available_functions.items()
        }
        self._dbg(f"[INIT] Async functions cached: {[k for k, v in self._async_function_flags.items() if v]}")


    def _dbg(self, msg: str):
        """ì‘ì€ ë””ë²„ê·¸ í—¬í¼: RAG ê´€ë ¨ ë‚´ë¶€ ìƒíƒœë¥¼ ë³´ê¸° ì‰½ê²Œ ì¶œë ¥."""
        if self.debug:
            print(f"[RAG-DEBUG] {msg}")

    def _format_rag_sources(self, source_documents: List[Dict[str, str]]) -> str:
        """RAG ë¬¸ì„œ ì¶œì²˜ë¥¼ í¬ë§·íŒ…í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        í˜•ì‹: "ì œëª© (íŒŒì¼ëª…)" - í™•ì¥ì ì œê±°
        
        Args:
            source_documents: ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            í¬ë§·íŒ…ëœ ì¶œì²˜ í…ìŠ¤íŠ¸ (ê° ë¬¸ì„œëŠ” ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)
        """
        if not source_documents:
            return ""
        
        formatted_sources = []
        seen = set()  # ì¤‘ë³µ ì œê±°ìš©
        
        for doc in source_documents:
            title = doc.get("title", "").strip()
            source_file = doc.get("source_file", "").strip()
            law_article_id = doc.get("law_article_id", "").strip()
            
            # ëª¨ë“  í•„ë“œê°€ ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤í‚µ
            if not title and not source_file and not law_article_id:
                continue
            
            # íŒŒì¼ í™•ì¥ì ì œê±°
            if source_file:
                # .hwp, .pdf, .docx ë“± í™•ì¥ì ì œê±°
                import re
                source_file = re.sub(r'\.(hwp|pdf|docx?|txt)$', '', source_file, flags=re.IGNORECASE)
            
            # ì¶œì²˜ ë¬¸ìì—´ ìƒì„±
            if title and source_file:
                source_str = f"{title} ({source_file})"
            elif title:
                source_str = title
            elif source_file:
                source_str = source_file
            else:
                source_str = law_article_id
            
            # ì¤‘ë³µ ì œê±°
            if source_str not in seen:
                seen.add(source_str)
                formatted_sources.append(f"  - {source_str}")
        
        return "\n".join(formatted_sources) if formatted_sources else ""

    def _extract_web_links(self, func_results: List[FunctionCallMetadata]) -> List[str]:
        """í•¨ìˆ˜ í˜¸ì¶œ ê²°ê³¼ì—ì„œ ì›¹ ê²€ìƒ‰ ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        search_internet í•¨ìˆ˜ì˜ output ì „ì²´ì—ì„œ Markdown ë§í¬ í˜•ì‹ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        - "ğŸ“ ì¶œì²˜:" ì„¹ì…˜ì´ ìˆìœ¼ë©´ ê·¸ê³³ì—ì„œ ìš°ì„  ì¶”ì¶œ
        - ì—†ìœ¼ë©´ ë³¸ë¬¸ ì „ì²´ì—ì„œ URL ë§í¬ ì¶”ì¶œ (LLMì´ ì§ì ‘ ì“´ ê²½ìš°)
        
        Args:
            func_results: í•¨ìˆ˜ í˜¸ì¶œ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì¶”ì¶œëœ ë§í¬ ë¦¬ìŠ¤íŠ¸ (Markdown í˜•ì‹ ìœ ì§€)
        """
        import re
        
        links = []
        seen_urls = set()  # ì¤‘ë³µ ì œê±°ìš©
        
        for meta in func_results:
            # search_internet í•¨ìˆ˜ë§Œ ì²˜ë¦¬
            if meta.name != "search_internet":
                continue
            
            output = meta.output if isinstance(meta.output, str) else str(meta.output)
            
            # [WEB_METADATA] ì œê±° (ë©”íƒ€ë°ì´í„°ëŠ” ë§í¬ ì¶”ì¶œ ëŒ€ìƒ ì•„ë‹˜)
            if "[WEB_METADATA]" in output:
                output = output.split("[WEB_METADATA]")[0]
            
            # "ğŸ“ ì¶œì²˜:" ì„¹ì…˜ì´ ìˆìœ¼ë©´ ìš°ì„  ì²˜ë¦¬
            if "ğŸ“ ì¶œì²˜:" in output:
                source_section = output.split("ğŸ“ ì¶œì²˜:")[-1]
                markdown_links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', source_section)
                
                for title, url in markdown_links:
                    if url not in seen_urls:
                        seen_urls.add(url)
                        links.append(f"[{title}]({url})")
            
            # ë³¸ë¬¸ ì „ì²´ì—ì„œë„ Markdown ë§í¬ ì¶”ì¶œ (LLMì´ ì§ì ‘ í¬í•¨ì‹œí‚¨ ê²½ìš°)
            # URLì´ ì‹¤ì œ ì›¹ ì£¼ì†Œì¸ì§€ í™•ì¸ (http:// ë˜ëŠ” https:// í¬í•¨)
            all_markdown_links = re.findall(r'\[([^\]]+)\]\((https?://[^)]+)\)', output)
            
            for title, url in all_markdown_links:
                if url not in seen_urls:
                    seen_urls.add(url)
                    links.append(f"[{title}]({url})")
        
        return links

    def _sanitize_function_arguments(self, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """
        í•¨ìˆ˜ í˜¸ì¶œ ì¸ìë¥¼ ë©”íƒ€ë°ì´í„° ì €ì¥ìš©ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.

        TokenCounterì™€ ê°™ì€ ì§ë ¬í™” ë¶ˆê°€ ê°ì²´ëŠ” ì œì™¸í•˜ê³ ,
        ê¸°íƒ€ ê°’ì€ JSONìœ¼ë¡œ ì§ë ¬í™”ê°€ ê°€ëŠ¥í•˜ë„ë¡ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """
        sanitized: Dict[str, Any] = {}

        for key, value in arguments.items():
            if key == "token_counter":
                continue  # ë‚´ë¶€ ìƒíƒœ ê°ì²´ëŠ” ê¸°ë¡í•˜ì§€ ì•ŠìŒ

            try:
                json.dumps(value, ensure_ascii=False)
                sanitized[key] = value
            except TypeError:
                sanitized[key] = str(value)

        return sanitized

    def _load_message_history(self, message_history: Optional[List[Dict[str, str]]]):
        """
        í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ ê¸°ì¡´ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ë¡œ ë¡œë“œ.

        Args:
            message_history: role/content í˜ì–´ ë¦¬ìŠ¤íŠ¸
        """
        self.context = [{"role": "system", "content": self.system_role}]

        history_items = message_history or []

        for item in history_items:
            if not isinstance(item, dict):
                continue

            role = item.get("role")
            content = item.get("content")

            if role not in ("user", "assistant"):
                continue
            if not isinstance(content, str):
                continue

            self.context.append({
                "role": role,
                "content": content,
            })

    def add_user_message_in_context(self, message: str):
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€:
          - ì‚¬ìš©ìê°€ ì…ë ¥í•œ messageë¥¼ contextì— user ì—­í• ë¡œ ì¶”ê°€
        """
        assistant_message = {
            "role": "user",
            "content": message,
        }
        if self.current_field == "main":
            self.context.append(assistant_message)

    #ì „ì†¡ë¶€
    def _send_request_Stream(self,temp_context=None):
        
        completed_text = ""

        if temp_context is None:
           current_context = self.get_current_context()
           openai_context = self.to_openai_context(current_context)
           stream = client.responses.create(
            model=self.model,
            input=openai_context,  
            top_p=1,
            stream=True,
            
            text={
                "format": {
                    "type": "text"  # ë˜ëŠ” "json_object" ë“± (Structured Output ì‚¬ìš© ì‹œ)
                }
            }
                )
        else:  
           stream = client.responses.create(
            model=self.model,
            input=temp_context,  # user/assistant ì—­í•  í¬í•¨ëœ list êµ¬ì¡°
            top_p=1,
            stream=True,
            text={
                "format": {
                    "type": "text"  # ë˜ëŠ” "json_object" ë“± (Structured Output ì‚¬ìš© ì‹œ)
                }
            }
                )
        
        loading = True  # deltaê°€ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ ë¡œë”© ì¤‘ ìƒíƒœ ìœ ì§€       
        for event in stream:
            #print(f"event: {event}")
            match event.type:
                case "response.created":
                    print("[ğŸ¤– ì‘ë‹µ ìƒì„± ì‹œì‘]")
                    loading = True
                    # ë¡œë”© ì• ë‹ˆë©”ì´ì…˜ìš© ëŒ€ê¸° ì‹œì‘
                    print("â³ GPTê°€ ì‘ë‹µì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤...")
                    
                case "response.output_text.delta":
                    if loading:
                        print("\n[ğŸ’¬ ì‘ë‹µ ì‹œì‘ë¨ â†“]")
                        loading = False
                    # ê¸€ì ë‹¨ìœ„ ì¶œë ¥
                    print(event.delta, end="", flush=True)
                 

                case "response.in_progress":
                    print("[ğŸŒ€ ì‘ë‹µ ìƒì„± ì¤‘...]")

                case "response.output_item.added":
                    if getattr(event.item, "type", None) == "reasoning":
                        print("[ğŸ§  GPTê°€ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤...]")
                    elif getattr(event.item, "type", None) == "message":
                        print("[ğŸ“© ë©”ì‹œì§€ ì•„ì´í…œ ì¶”ê°€ë¨]")
                #ResponseOutputItemDoneEventëŠ” ìš°ë¦¬ê°€ case "response.output_item.done"ì—ì„œ ì¡ì•„ì•¼ í•´
                case "response.output_item.done":
                    item = event.item
                    if item.type == "message" and item.role == "assistant":
                        for part in item.content:
                            if getattr(part, "type", None) == "output_text":
                                completed_text= part.text
                case "response.completed":
                    print("\n")
                    #print(f"\nğŸ“¦ ìµœì¢… ì „ì²´ ì¶œë ¥: \n{completed_text}")
                case "response.failed":
                    print("âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
                case "error":
                    print("âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì—ëŸ¬ ë°œìƒ!")
                case _:
                    
                    print(f"[ğŸ“¬ ê¸°íƒ€ ì´ë²¤íŠ¸ ê°ì§€: {event.type}]")
        return completed_text
  
  
    def send_request_Stream(self):
      self.context[-1]['content']+=self.instruction
      return self._send_request_Stream()
    #ì±—ë´‡ì— ë§ê²Œ ë¬¸ë§¥ íŒŒì‹±
    def add_response(self, response):
        response_message = {
            "role" : response['choices'][0]['message']["role"],
            "content" : response['choices'][0]['message']["content"],            
        }
        self.context.append(response_message)

    def add_response_stream(self, response):
            """
                ì±—ë´‡ ì‘ë‹µì„ í˜„ì¬ ëŒ€í™”ë°©ì˜ ë¬¸ë§¥ì— ì¶”ê°€í•©ë‹ˆë‹¤.
                
                Args:
                    response (str): ì±—ë´‡ì´ ìƒì„±í•œ ì‘ë‹µ í…ìŠ¤íŠ¸.
                """
            assistant_message = {
            "role": "assistant",
            "content": response,
           
        }
            self.context.append(assistant_message)

    def get_response(self, response_text: str):
        """
        ì‘ë‹µë‚´ìš©ë°˜í™˜:
          - ë©”ì‹œì§€ë¥¼ ì½˜ì†”(ë˜ëŠ” UI) ì¶œë ¥ í›„, ê·¸ëŒ€ë¡œ ë°˜í™˜
        """
        print(response_text['choices'][0]['message']['content'])
        return response_text
#ë§ˆì§€ë§‰ ì§€ì¹¨ì œê±°
    def clean_context(self):
        '''
        1.contextë¦¬ìŠ¤íŠ¸ì— ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ë¶€í„° ì²˜ìŒê¹Œì§€ ìˆœíšŒí•œë‹¤
        2."instruction:\n"ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ìì—´ì„ ë‚˜ëˆˆë‹¤..ì²«userì„ ì°¾ìœ¼ë©´ ì•„ë˜ ê³¼ì •ì„ ì§„í–‰í•œë‹¤,
        3.ì²« ë²ˆì§¸ ë¶€ë¶„ [0]ë§Œ ê°€ì ¸ì˜¨ë‹¤. (ì¦‰, "instruction:\n" ì´ì „ì˜ ë¬¸ìì—´ë§Œ ë‚¨ê¸´ë‹¤.)
        4.strip()ì„ ì ìš©í•˜ì—¬ ì•ë’¤ì˜ ê³µë°±ì´ë‚˜ ê°œí–‰ ë¬¸ìë¥¼ ì œê±°í•œë‹¤.
        '''
        for idx in reversed(range(len(self.context))):
            if self.context[idx]['role']=='user':
                self.context[idx]["content"]=self.context[idx]['content'].split('instruction:\n')[0].strip()
                break
#ì§ˆì˜ì‘ë‹µ í† í° ê´€ë¦¬
    def handle_token_limit(self, response):
        # ëˆ„ì  í† í° ìˆ˜ê°€ ì„ê³„ì ì„ ë„˜ì§€ ì•Šë„ë¡ ì œì–´í•œë‹¤.
        try:
            current_usage_rate = response['usage']['total_tokens'] / self.max_token_size
            exceeded_token_rate = current_usage_rate - self.available_token_rate
            if exceeded_token_rate > 0:
                remove_size = math.ceil(len(self.context) / 10)
                self.context = [self.context[0]] + self.context[remove_size+1:]
        except Exception as e:
            print(f"handle_token_limit exception:{e}")
            
    def to_openai_context(self, context):
        return [{"role":v["role"], "content":v["content"]} for v in context]

    def get_current_context(self):
        """í˜„ì¬ ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜"""
        return self.context

    def get_rag_context(self, user_question: str):
        """RAG ì»¨í…ìŠ¤íŠ¸ë§Œ ì¤€ë¹„í•˜ì—¬ ë°˜í™˜ (ì—†ìœ¼ë©´ None). ì—¬ê¸°ì„œë¶€í„° ì‚¬ìš©ì ë©”ì‹œì§€ëŠ” ì§ˆë¬¸ìœ¼ë¡œ ì·¨ê¸‰."""
        result = self.rag_service.retrieve_context(user_question)
        self._last_rag_result = result
        return result.merged_documents_text

    @property
    def last_rag_result(self):
        """ìµœê·¼ RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë…¸ì¶œí•©ë‹ˆë‹¤ (ì—†ìœ¼ë©´ None)."""
        return self._last_rag_result or self.rag_service.last_result
    def get_response_from_db_only(self, user_question: str):
        self._dbg("get_response_from_db_only: start")
        rag_result = self.rag_service.retrieve_context(user_question)
        rag_context = rag_result.merged_documents_text
        if rag_context is None:
            self._dbg("ê¸°ì–µê²€ìƒ‰ ì•„ë‹˜")
            return False

        # LLM í˜¸ì¶œí•  context êµ¬ì„±: system ë©”ì‹œì§€ + DB ë‚´ìš©(system role) + user ì§ˆë¬¸
        context = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ í•™ì‚¬ ê·œì • ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."},
            {"role": "system", "content": rag_context},
            {"role": "user", "content": user_question},
        ]
        self._dbg(
            f"get_response_from_db_only: messages=[system, system(ctx:{len(rag_context)} chars), user] model={self.model}"
        )

        return self._send_request_Stream(temp_context=self.to_openai_context(context))

    # ==========================================
    # Phase 2: í—¬í¼ ë©”ì„œë“œ êµ¬í˜„ (routes.py â†’ stream.py)
    # ==========================================

    def _get_language_instruction(self, language: str) -> str:
        """
        ì–¸ì–´ ì½”ë“œì— ë”°ë¥¸ ì‘ë‹µ ì§€ì¹¨ ë°˜í™˜
        
        Args:
            language: ì–¸ì–´ ì½”ë“œ (KOR, ENG, VI, JPN, CHN, UZB, MNG, IDN)
        
        Returns:
            str: í•´ë‹¹ ì–¸ì–´ì— ë§ëŠ” ì‘ë‹µ ì§€ì¹¨
        """
        instruction_map = {
            "KOR": "í•œêµ­ì–´ë¡œ ì •ì¤‘í•˜ê³  ë”°ëœ»í•˜ê²Œ ë‹µí•´ì£¼ì„¸ìš”.",
            "ENG": "Please respond kindly in English.",
            "VI": "Vui lÃ²ng tráº£ lá»i báº±ng tiáº¿ng Viá»‡t má»™t cÃ¡ch nháº¹ nhÃ ng.",
            "JPN": "æ—¥æœ¬èªã§ä¸å¯§ã«æ¸©ã‹ãç­”ãˆã¦ãã ã•ã„ã€‚",
            "CHN": "è¯·ç”¨ä¸­æ–‡äº²åˆ‡åœ°å›ç­”ã€‚",
            "UZB": "Iltimos, o'zbek tilida samimiy va hurmat bilan javob bering.",
            "MNG": "ĞœĞ¾Ğ½Ğ³Ğ¾Ğ» Ñ…ÑĞ»ÑÑÑ€ ÑĞµĞ»Ğ´ÑĞ³, Ğ´ÑƒĞ»Ğ°Ğ°Ñ…Ğ°Ğ½ Ñ…Ğ°Ñ€Ğ¸ÑƒĞ»Ğ½Ğ° ÑƒÑƒ.",
            "IDN": "Tolong jawab dengan ramah dan hangat dalam bahasa Indonesia."
        }
        return instruction_map.get(language, instruction_map["KOR"])

    async def _condense_rag_context(self, user_question: str, raw_context: str) -> str:
        """
        ê¸´ RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ê²Œ ìš”ì•½
        
        - í‘œ/ë²ˆí˜¸ì¡°í•­ì˜ ì „ì²´ ë§¥ë½ í¬í•¨ (ìµœì†Œ 15ì¤„)
        - ì£¼ì„(ì£¼) í¬í•¨
        - ì›ë¬¸ êµ¬ì¡° ìœ ì§€
        
        Args:
            user_question: ì‚¬ìš©ì ì§ˆë¬¸
            raw_context: ì›ë³¸ RAG ì»¨í…ìŠ¤íŠ¸
        
        Returns:
            str: ìš”ì•½ëœ ì»¨í…ìŠ¤íŠ¸ (ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì¼ë¶€ ë°˜í™˜)
        """
        self._dbg(f"[CONDENSE] ì‹œì‘ - ì›ë³¸ ê¸¸ì´: {len(raw_context)}ì, ì§ˆë¬¸: {user_question[:50]}...")
        
        def _sanitize_text(txt: str) -> str:
            """ì œì–´ë¬¸ì ì œê±° ë° íƒœê·¸ ì¶©ëŒ ë°©ì§€"""
            if not isinstance(txt, str):
                txt = str(txt)
            # ê°„ë‹¨ ì œì–´ë¬¸ì í•„í„°ë§ (LF, TAB ì œì™¸)
            txt = ''.join(ch for ch in txt if ch in ('\n', '\t') or (ord(ch) >= 32 and ch != 127))
            # íƒœê·¸ ì¡°ê¸° ì¢…ë£Œ ë°©ì§€
            txt = txt.replace("</ê¸°ì–µê²€ìƒ‰>", "[/ê¸°ì–µê²€ìƒ‰]")
            # ê³¼ë„í•œ ê¸¸ì´ í´ë¨í”„
            max_len = 30000
            return txt[:max_len]

        sanitized_rag = _sanitize_text(raw_context)
        self._dbg(f"[CONDENSE] Sanitized ê¸¸ì´: {len(sanitized_rag)}ì")

        condense_prompt = [
            {
                "role": "system",
                "content": (
                                    f"""
                ë‹¹ì‹ ì€ ê¸´ ê·œì •/ì„¸ì¹™ ë¬¸ì„œ ë¬¶ìŒì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë¶€ë¶„ì„ "ë„“ì€ ë§¥ë½"ìœ¼ë¡œ ì¶”ì¶œÂ·í‘œì‹œí•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                ê·œì¹™(ë„“ì€ ë§¥ë½ í¬í•¨):
                1) ì›ë¬¸ ì „ì²´ëŠ” <ê¸°ì–µê²€ìƒ‰> íƒœê·¸ ì•ˆì— ìˆìŠµë‹ˆë‹¤.
                2) ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ê·¼ê±°ëŠ” <ë°˜ì˜>...</ë°˜ì˜> íƒœê·¸ ì•ˆì— ë‹´ë˜, ë‹¤ìŒì„ í¬í•¨í•˜ì„¸ìš”.
                - í‘œ/ëª©ë¡/ë²ˆí˜¸ ì¡°í•­ì€ í•´ë‹¹ í•­ëª©ì˜ ë¨¸ë¦¬ê¸€(ì œëª©/í—¤ë”)ê³¼ ì¸ì ‘ í–‰Â·í•­ê¹Œì§€ í•¨ê»˜ í¬í•¨(ìµœì†Œ Â±5~10ì¤„ ë§¥ë½).
                - "ì£¼)" í˜•íƒœì˜ ì£¼ì„/ë¹„ê³ ê°€ ë¶™ì€ ê²½ìš° í•´ë‹¹ ì£¼ì„ ì „ë¶€ í¬í•¨.
                - í•™ì Â·ê³¼ëª©Â·ë°°ë¶„ì˜ì—­Â·íŠ¸ë™ê³¼ ê°™ì€ ìˆ«ì/í•­ëª©ì€ í‘œì˜ ì—´ ë¨¸ë¦¬ë§ê³¼ ê°™ì´ í¬í•¨(í—¤ë”+í–‰ ì„¸íŠ¸).
                3) ì‚¬ìš©ìê°€ íŠ¹ì • ë²ˆí˜¸(ì˜ˆ: 1ë²ˆ, 2ë²ˆ)ë¥¼ ì–¸ê¸‰í–ˆì§€ë§Œ ëª¨í˜¸í•  ê²½ìš°, í›„ë³´ ë²ˆí˜¸ 2~3ê°œë¥¼ ëª¨ë‘ í¬í•¨í•˜ë˜ ê° ë¸”ë¡ ì•ì— [í›„ë³´] í‘œê¸°.
                4) ê´€ë ¨ ê·¼ê±°ê°€ ì¶©ë¶„ì¹˜ ì•Šë‹¤ê³  íŒë‹¨ë˜ë©´, ìƒìœ„ ë‹¨ë½(ì¡°/í•­/í‘œ ì œëª©) ë‹¨ìœ„ê¹Œì§€ í™•ì¥í•˜ì—¬ ìµœì†Œ 15ì¤„ ì´ìƒì„ ë‹´ê³ , ì§€ë‚˜ì¹œ ìš”ì•½ì„ í”¼í•˜ì„¸ìš”.
                5) ì›ë¬¸ êµ¬ì¡°(ì¡°/í•­/í˜¸/í‘œ ì œëª©)ëŠ” ìœ ì§€í•˜ê³  ì„ì˜ ì¬ì‘ì„± ê¸ˆì§€. ë°˜ë“œì‹œ ì›ë¬¸ì„ ê±°ì˜ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì„¸ìš”.
                6) ì›ë¬¸ ë°– ì¶”ë¡ /ì°½ì‘ ê¸ˆì§€.

                ì‚¬ìš©ì ì§ˆë¬¸: {user_question}
                <ê¸°ì–µê²€ìƒ‰>{sanitized_rag}</ê¸°ì–µê²€ìƒ‰>
                """
                ),
            }
        ]
        
        self._dbg("[CONDENSE] 1ì°¨ ìš”ì•½ ì‹œë„ ì¤‘...")

        try:
            # LLM Manager ì‚¬ìš© (êµì²´ ê°€ëŠ¥)
            provider = get_provider("condense")
            condensed, usage1 = await provider.simple_completion(condense_prompt)
            condensed = condensed.strip()

            # âœ… 1ì°¨ ì‹œë„ API usage ë°˜ì˜
            if usage1:
                self.token_counter.update_from_api_usage(
                    usage=usage1,
                    role="condense",
                    model=provider.get_model_name(),
                    category="rag"
                )

            self._dbg(f"[CONDENSE] 1ì°¨ ê²°ê³¼ - ê¸¸ì´: {len(condensed)}ì, ì¤„ ìˆ˜: {condensed.count(chr(10))}ì¤„")

            # 1ì°¨ ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ 2ì°¨ ì‹œë„
            if (len(condensed) <200):
                self._dbg("[CONDENSE] 1ì°¨ ê²°ê³¼ ë„ˆë¬´ ì§§ìŒ -> 2ì°¨ ì‹œë„ (ë„“ì€ ë§¥ë½)")
                broader_prompt = [
                    {
                        "role": "system",
                        "content": (
                                                    f"""
                        ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í‘œ/ë²ˆí˜¸ì¡°í•­/ì£¼ì„ì˜ ì „ì²´ ë§¥ë½ì„ ë„“ê²Œ í¬í•¨í•´ ì¶”ì¶œí•©ë‹ˆë‹¤.
                        ë°˜ë“œì‹œ ë‹¤ìŒì„ ì§€í‚¤ì„¸ìš”:
                        - <ë°˜ì˜>...</ë°˜ì˜> ì•ˆì— í—¤ë”(í‘œ ì œëª©/ì—´ ë¨¸ë¦¬ë§) + ê´€ë ¨ í–‰/í•­ ì „ë¶€ì™€ í•´ë‹¹ ì£¼ì„(ì£¼)ê¹Œì§€ í¬í•¨.
                        - ìµœì†Œ 25ì¤„ ì´ìƒ, ê°€ëŠ¥í•˜ë©´ ê´€ë ¨ ë¸”ë¡ì„ í†µì§¸ë¡œ í¬í•¨(ë¶ˆí•„ìš”í•œ ìš”ì•½ ê¸ˆì§€).
                        - ëª¨í˜¸í•˜ë©´ í›„ë³´ ë¸”ë¡ 2~3ê°œë¥¼ [í›„ë³´]ë¡œ ë‚˜ëˆ„ì–´ ëª¨ë‘ í¬í•¨.
                        ì›ë¬¸: <ê¸°ì–µê²€ìƒ‰>{sanitized_rag}</ê¸°ì–µê²€ìƒ‰>
                        ì§ˆë¬¸: {user_question}
                        """
                        ),
                    }
                ]
                try:
                    self._dbg("[CONDENSE] 2ì°¨ ìš”ì•½ ì‹œë„ ì¤‘...")
                    # LLM Manager ì‚¬ìš© (êµì²´ ê°€ëŠ¥)
                    condensed2, usage2 = await provider.simple_completion(broader_prompt)
                    condensed2 = condensed2.strip()

                    # âœ… 2ì°¨ ì‹œë„ API usage ë°˜ì˜
                    if usage2:
                        self.token_counter.update_from_api_usage(
                            usage=usage2,
                            role="condense",
                            model=provider.get_model_name(),
                            category="rag"
                        )

                    self._dbg(f"[CONDENSE] 2ì°¨ ê²°ê³¼ - ê¸¸ì´: {len(condensed2)}ì, ì¤„ ìˆ˜: {condensed2.count(chr(10))}ì¤„")

                    # ë” ê¸¸ê³  í’ë¶€í•˜ë©´ êµì²´
                    if (condensed2.count("\n") >= condensed.count("\n")) and (len(condensed2) > len(condensed)):
                        self._dbg("[CONDENSE] 2ì°¨ ê²°ê³¼ê°€ ë” í’ë¶€í•¨ -> êµì²´")
                        condensed = condensed2
                    else:
                        self._dbg("[CONDENSE] 1ì°¨ ê²°ê³¼ ìœ ì§€")
                except Exception as e2:
                    self._dbg(f"[CONDENSE] 2ì°¨ ì‹œë„ ì‹¤íŒ¨: {e2}")

            self._dbg(f"[CONDENSE] ìµœì¢… ê²°ê³¼ - ê¸¸ì´: {len(condensed)}ì")
            
            return condensed
            
        except Exception as e:
            # ìš”ì•½ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ì„ ì§§ê²Œ ì˜ë¼ ì‚¬ìš©
            self._dbg(f"[CONDENSE] ë¬¸ì„œ ìš”ì•½ ì‹¤íŒ¨: {e}")
            fallback = sanitized_rag[:6000]
            self._dbg(f"[CONDENSE] Fallback ì‚¬ìš© - ê¸¸ì´: {len(fallback)}ì")
            return fallback

    def _build_final_context(
        self,
        message: str,
        condensed_rag: Optional[str],
        func_results: List[FunctionCallMetadata],
        language: str = "KOR",
    ) -> List[Dict[str, str]]:
        """ìµœì¢… LLM ì…ë ¥ ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

        ì‚¬ìš©ì ì§ˆë¬¸, ìš”ì•½ëœ ê¸°ì–µê²€ìƒ‰ ê²°ê³¼, í•¨ìˆ˜ í˜¸ì¶œ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬
        ë‹¨ì¼ ì‹œìŠ¤í…œ ë©”ì‹œì§€ í˜•íƒœë¡œ ì •ë¦¬í•œ ë’¤ ê¸°ì¡´ ëŒ€í™”ë¬¸ë§¥ì— ì¶”ê°€í•©ë‹ˆë‹¤.

        Args:
            message: í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸.
            condensed_rag: LLMìœ¼ë¡œ ê°€ê³µëœ ê¸°ì–µê²€ìƒ‰ ìš”ì•½ ë¬¸ìì—´. ì—†ìœ¼ë©´ None.
            func_results: í•¨ìˆ˜ í˜¸ì¶œ ë©”íƒ€ë°ì´í„° ëª©ë¡.
            language: ì‘ë‹µ ì–¸ì–´ ì½”ë“œ (KOR, ENG, VI, JPN, CHN, UZB, MNG, IDN).

        Returns:
            OpenAI Responses APIì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸.
        """

        base_context = self.to_openai_context(self.context[:])
        has_rag = bool(condensed_rag and condensed_rag.strip())
        has_funcs = bool(func_results)

        sections: List[str] = []

        # 0) í˜„ì¬ ë‚ ì§œ/ì‹œê°„
        current_datetime = currTime()  # "2025.11.15 14:30:25" í˜•ì‹
        sections.append(
            f"[í˜„ì¬ ë‚ ì§œ/ì‹œê°„]\n{current_datetime}\n"
            "ì´ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ 'ìµœì‹ ', 'ìš”ì¦˜', 'ìµœê·¼' ë“±ì˜ í‘œí˜„ì„ í•´ì„í•˜ì„¸ìš”.\n"
            "**ì¤‘ìš”**: 'ê³µì§€ì‚¬í•­'ì´ë¼ëŠ” ë‹¨ì–´ë§Œ ìˆì–´ë„ ì‚¬ìš©ìëŠ” í˜„ì¬ ì‹œì ì˜ ìµœì‹  ê³µì§€ì‚¬í•­ì„ ì›í•˜ëŠ” ê²ƒìœ¼ë¡œ ì´í•´í•˜ì„¸ìš”."
        )

        # 1) ì‚¬ìš©ì ì¿¼ë¦¬ ì§€ì¹¨
        query_guidance = (
            f"ì´ê²ƒì€ ì‚¬ìš©ì ì¿¼ë¦¬ì…ë‹ˆë‹¤: {message}\n"
            "ë‹¤ìŒ ì •ë³´ë¥¼ [ì‚¬ìš©ìì¿¼ë¦¬ì§€ì¹¨],[ì¼ë°˜ì§€ì¹¨],[ê¸°ì–µê²€ìƒ‰ì§€ì¹¨],[ì›¹ê²€ìƒ‰ì§€ì¹¨] ë“± ì„œìˆ ëœ ì„œìˆ ê³¼ ì§€ì¹¨ì— ë”°ë¼ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ëŒ€ë‹µì— ë§ê²Œ í†µí•©í•´ ì „ë‹¬í•˜ì„¸ìš”.\n"
            "- í•¨ìˆ˜í˜¸ì¶œ ê²°ê³¼: ìˆìœ¼ë©´ ë°˜ì˜\n"
            "- ê¸°ì–µê²€ìƒ‰ ê²°ê³¼: ìˆìœ¼ë©´ ë°˜ì˜ / í•¨ìˆ˜ í˜¸ì¶œ ì¡´ì¬ ìì²´ëŠ” ì–¸ê¸‰ ê¸ˆì§€"
        )
        sections.append("[ì‚¬ìš©ìì¿¼ë¦¬ì§€ì¹¨]\n" + query_guidance)

        # 2) ì¼ë°˜ ì§€ì¹¨
        sections.append("[ì¼ë°˜ì§€ì¹¨]\n" + self.instruction)

        # 3) ê¸°ì–µê²€ìƒ‰ ì§€ì¹¨ ë° ë³¸ë¬¸
        if has_rag:
            rag_guidance = (
                "ê¸°ì–µê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. <ë°˜ì˜> </ë°˜ì˜> íƒœê·¸ ë‚´ë¶€ ë‚´ìš©ì„ ë³´ê³  ì‚¬ìš©ìì˜ ì›í•˜ëŠ” ì¿¼ë¦¬ì— ë§ê²Œ ëŒ€ë‹µí•˜ì„¸ìš”. "
                "<ê¸°ì–µê²€ìƒ‰></ê¸°ì–µê²€ìƒ‰> íƒœê·¸ëŠ” ì°¸ì¡°ìš©ì´ë©° íƒœê·¸ ë°– ì„ì˜ ì°½ì‘ ê¸ˆì§€"
            )
            sections.append("[ê¸°ì–µê²€ìƒ‰ì§€ì¹¨]\n" + rag_guidance)
            sections.append("[ê¸°ì–µê²€ìƒ‰]\n<ê¸°ì–µê²€ìƒ‰>\n" + condensed_rag + "\n</ê¸°ì–µê²€ìƒ‰>")

        web_status: Optional[str] = None

        if has_funcs:
            formatted_blocks_web: List[str] = []
            formatted_blocks_other: List[str] = []
            web_outputs: List[str] = []

            for meta in func_results:
                args_str: str
                try:
                    args_str = json.dumps(meta.arguments, ensure_ascii=False)
                except (TypeError, ValueError):
                    args_str = str(meta.arguments)

                out_text = meta.output if isinstance(meta.output, str) else str(meta.output)
                
                # search_internet í•¨ìˆ˜ì˜ ê²½ìš° ë§í¬ ì„¹ì…˜ ì œê±° (ì¤‘ë³µ ë°©ì§€)
                if meta.name == "search_internet" and "ğŸ“ ì¶œì²˜:" in out_text:
                    # "ğŸ“ ì¶œì²˜:" ì´ì „ê¹Œì§€ë§Œ ì‚¬ìš© (ë§í¬ëŠ” ë‚˜ì¤‘ì— ë³„ë„ë¡œ í‘œì‹œ)
                    out_text = out_text.split("ğŸ“ ì¶œì²˜:")[0].strip()
                
                max_len = 4000
                if len(out_text) > max_len:
                    out_text = out_text[:max_len] + "...<truncated>"

                block = f"<function name='{meta.name}' args='{args_str}'>\n{out_text}\n</function>"

                if meta.name == "search_internet":
                    web_outputs.append(out_text)
                    formatted_blocks_web.append(block)
                else:
                    formatted_blocks_other.append(block)

            web_functions_block = "\n".join(formatted_blocks_web) if formatted_blocks_web else ""
            other_functions_block = "\n".join(formatted_blocks_other) if formatted_blocks_other else ""

            if not web_outputs:
                web_status = "not-run"
            else:
                def _is_error_or_empty(txt: str) -> bool:
                    t = (txt or "").strip()
                    if not t:
                        return True
                    err_keywords = [
                        "ğŸš¨",
                        "âŒ",
                        "ì˜¤ë¥˜",
                        "error",
                        "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜",
                        "no result",
                        "did_call=False",
                    ]
                    return any(k.lower() in t.lower() for k in err_keywords)

                all_bad = all(_is_error_or_empty(t) for t in web_outputs)
                web_status = "empty-or-error" if all_bad else "ok"

            web_guidance = (
                "ë‹¤ìŒì€ ì¸í„°ë„· ê²€ìƒ‰ê²°ê³¼ì…ë‹ˆë‹¤. ê³µì‹ ê·¼ê±°ê°€ ì•„ë‹ˆë¯€ë¡œ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”. "
                "ê²€ìƒ‰ì´ ì•ˆë˜ì–´ ìš°íšŒ/ë¬¸ì˜ ì•ˆë‚´ë§Œ ìˆì„ ê²½ìš°, ë¬´ì‹œí•˜ê³  ì´ ë‚´ìš©ì€'ì°¸ì¡°ë§Œ' í•˜ì„¸ìš”. ë°˜ë“œì‹œ ê¸°ì–µê²€ìƒ‰ ê·¼ê±°ë¥¼ ìš°ì„  ë°˜ì˜í•˜ì„¸ìš”. ì°¸ì¡°ë€ ì•ˆë‚´ ì „í™”ë²ˆí˜¸ ì‚¬ì´íŠ¸ë§Œì„ ë°˜ì˜í•˜ëŠ”ê²ƒì„ ë§í•©ë‹ˆë‹¤ "
            )
            sections.append("[ì›¹ê²€ìƒ‰ì§€ì¹¨]\n" + web_guidance)
            if web_functions_block:
                sections.append("[ì¸í„°ë„· ê²€ìƒ‰ê²°ê³¼]\n<ì¸í„°ë„·ê²€ìƒ‰>\n" + web_functions_block + "\n</ì¸í„°ë„·ê²€ìƒ‰>")

            func_guidance = (
                "ë‹¤ìŒì€ í•¨ìˆ˜(ê²€ìƒ‰/ë©”ë‰´ ë“±) í˜¸ì¶œ ê²°ê³¼ì…ë‹ˆë‹¤. <í•¨ìˆ˜ê²°ê³¼> íƒœê·¸ ë‚´ë¶€ ë‚´ìš©ì€ ì°¸ê³ ìš©ì´ë©°, ë°˜ë“œì‹œ ì•„ë˜ ê¸°ì–µê²€ìƒ‰(<ê¸°ì–µê²€ìƒ‰>) ê·¼ê±°ë¥¼ ìš°ì„  ë‹µë³€ì— ë°˜ì˜í•˜ì„¸ìš”. "
                "'í•¨ìˆ˜ í˜¸ì¶œ'ì´ë¼ëŠ” í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ê³ , ê±°ì§“ ì •ë³´ ìƒì„± ê¸ˆì§€."
            )
            sections.append("[í•¨ìˆ˜ê²°ê³¼ì§€ì¹¨]\n" + func_guidance)
            if other_functions_block:
                sections.append("[í•¨ìˆ˜ê²°ê³¼]\n<í•¨ìˆ˜ê²°ê³¼>\n" + other_functions_block + "\n</í•¨ìˆ˜ê²°ê³¼>")

            if web_status and web_status != "not-run":
                status_kr = {
                    "ok": "ì •ìƒ",
                    "empty-or-error": "ê²°ê³¼ì—†ìŒ/ì˜¤ë¥˜",
                    "not-run": "ì‹¤í–‰ì•ˆí•¨",
                }[web_status]
                sections.append("[ì›¹ê²€ìƒ‰ìƒíƒœ]\n" + status_kr)

            if web_status in ("empty-or-error", "not-run"):
                if has_rag:
                    sections.append(
                        "[ì›¹ê²€ìƒ‰ê²°ê³¼ì—†ìŒì§€ì¹¨]\nì¸í„°ë„· ê²€ìƒ‰ê²°ê³¼ëŠ” ì°¸ê³ ìš©ì…ë‹ˆë‹¤. ê³µì‹ ê·œì •ì€ ì•„ë˜ ê¸°ì–µê²€ìƒ‰(<ê¸°ì–µê²€ìƒ‰>) ê·¼ê±°ë¥¼ ë°˜ë“œì‹œ ìš°ì„  í™•ì¸í•˜ì„¸ìš”. ê²€ìƒ‰ì´ ë˜ì§€ ì•Šê±°ë‚˜ ë¬¸ì˜ ì•ˆë‚´ë§Œ ìˆì„ ê²½ìš°, í•´ë‹¹ ë‚´ìš©ì€ ì°¸ê³ ë§Œ í•˜ì‹œê³  ë°˜ë“œì‹œ ì•„ë˜ ê·œì • ê·¼ê±°ë¥¼ ë‹µë³€ì— ë°˜ì˜í•˜ì„¸ìš”."
                    )
                else:
                    sections.append(
                        "[ì›¹ê²€ìƒ‰ê²°ê³¼ì—†ìŒì§€ì¹¨]\nì›¹ê²€ìƒ‰ ê²°ê³¼ëŠ” ì—†ì—ˆìŠµë‹ˆë‹¤. ê´€ë ¨ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŒì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨íˆ ì•Œë¦¬ê³ , í•„ìš”í•œ ì¶”ê°€ ì •ë³´ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì²­í•˜ì„¸ìš”."
                    )

        if has_rag and has_funcs:
            extra_note = ""
            if web_status == "empty-or-error":
                extra_note = " ì›¹ê²€ìƒ‰ì´ ê²°ê³¼ì—†ìŒ/ì˜¤ë¥˜ì—¬ë„ ê¸°ì–µê²€ìƒ‰ì´ ì¡´ì¬í•˜ë©´ 'ì •ë³´ ì—†ìŒ'ì´ë¼ê³  í•˜ì§€ ë§ê³  ê¸°ì–µê²€ìƒ‰ ê·¼ê±°ë¡œ ë‹µí•  ê²ƒ."
            merge_instruction = (
                "ìœ„ ê¸°ì–µê²€ìƒ‰ ê·¼ê±°(<ê¸°ì–µê²€ìƒ‰>)ì™€ ì¸í„°ë„· ê²€ìƒ‰ê²°ê³¼(<ì¸í„°ë„·ê²€ìƒ‰>), ê¸°íƒ€ í•¨ìˆ˜ê²°ê³¼(<í•¨ìˆ˜ê²°ê³¼>)ë¥¼ ëŒ€ì¡°í•˜ì—¬ ëª¨ìˆœ ì—†ì´ ë‹µí•˜ì„¸ìš”. "
                "í•µì‹¬ ë‹µ ë¨¼ì € ì œì‹œí•˜ê³ , í•„ìš”í•œ ê·¼ê±°ë§Œ ì¶•ì•½ ì¸ìš©. ì¸í„°ë„· ê²€ìƒ‰ê²°ê³¼ëŠ” ì°¸ê³ ìš©ì´ë©°, ìš°íšŒ/ë¬¸ì˜ ì•ˆë‚´ë§Œ ìˆì„ ê²½ìš° 'ì°¸ì¡°ë§Œ' í•˜ê³  -ì°¸ì¡°ë€ ì•ˆë‚´ ì „í™”ë²ˆí˜¸ ì‚¬ì´íŠ¸ë§Œì„ ë°˜ì˜í•˜ëŠ”ê²ƒì„ ë§í•©ë‹ˆë‹¤   "
                "ë°˜ë“œì‹œ ê¸°ì–µê²€ìƒ‰ ê·¼ê±°ë¥¼ ìš°ì„  ë°˜ì˜í•˜ì„¸ìš”. ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œ." + extra_note
            )
            sections.append("[í†µí•©ì§€ì¹¨]\n" + merge_instruction)

        # ì–¸ì–´ ì§€ì¹¨ ì¶”ê°€ (í•­ìƒ ë§ˆì§€ë§‰ì— ì¶”ê°€)
        language_instruction = self._get_language_instruction(language)

        # ì¶”ê°€ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ì— ì–¸ì–´ ì§€ì¹¨ë§Œ ì¶”ê°€
        if not (has_rag or has_funcs):
            base_context.append({
                "role": "system",
                "content": f"[ì–¸ì–´ì§€ì¹¨]\n{language_instruction}",
            })
            self._last_web_status = web_status
            return base_context

        # ì¶”ê°€ ì •ë³´ê°€ ìˆìœ¼ë©´ sections ë§ˆì§€ë§‰ì— ì–¸ì–´ ì§€ì¹¨ ì¶”ê°€
        sections.append(f"[ì–¸ì–´ì§€ì¹¨]\n{language_instruction}")

        base_context.append({
            "role": "system",
            "content": "\n\n".join(sections),
        })

        self._last_web_status = web_status
        return base_context

    async def _analyze_and_execute_functions(
        self, 
        message: str
    ) -> tuple[str | None, List[FunctionCallMetadata]]:
        """í•¨ìˆ˜ í˜¸ì¶œ ë¶„ì„ ë° ì‹¤í–‰
        
        1) FunctionCalling.analyze()ë¡œ í•„ìš”í•œ í•¨ìˆ˜ íŒŒì•… (ì¶”ë¡  í¬í•¨)
        2) ê° í•¨ìˆ˜ ì‹¤í–‰
        3) í•™ì‹ í‚¤ì›Œë“œ ê¸°ë°˜ fallback í˜¸ì¶œ(ê·œì¹™ ê¸°ë°˜) í¬í•¨
        4) ê²°ê³¼ë¥¼ FunctionCallMetadata ë¦¬ìŠ¤íŠ¸ë¡œ ì§ë ¬í™”í•˜ì—¬ ë°˜í™˜
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            
        Returns:
            tuple: (ì¶”ë¡  í…ìŠ¤íŠ¸, í•¨ìˆ˜ í˜¸ì¶œ ë©”íƒ€ë°ì´í„° ëª©ë¡)
        """
        func_results: List[FunctionCallMetadata] = []
        
        # 1) í•¨ìˆ˜ ë¶„ì„ (ì¶”ë¡  + í•¨ìˆ˜ í˜¸ì¶œ ëª©ë¡)
        analyze_result = await self.func_calling.analyze(message, self.tools)
        reasoning = analyze_result.get("reasoning")
        analyzed = analyze_result.get("output", [])
        
        # reasoningì—ì„œ ì„ íƒëœ ë„êµ¬ ëª©ë¡ ì¶”ì¶œ
        selected_tools_from_reasoning = []
        try:
            # analyze_resultì— selected_toolsê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if "selected_tools" in analyze_result:
                selected_tools_from_reasoning = analyze_result.get("selected_tools", [])
        except Exception:
            pass
        
        # OpenAI APIì—ì„œ function_callì„ ë°˜í™˜í•œ ê²½ìš° ì²˜ë¦¬
        function_calls_found = False
        for tool_call in analyzed:
            if getattr(tool_call, "type", None) != "function_call":
                continue
            
            function_calls_found = True
                
            func_name = tool_call.name
            call_id = getattr(tool_call, "call_id", "call_unknown")
            
            # ì¸ì íŒŒì‹±
            try:
                func_args = json.loads(tool_call.arguments)
            except (json.JSONDecodeError, TypeError) as e:
                self._dbg(f"[FUNCTION] ì¸ì íŒŒì‹± ì‹¤íŒ¨: {func_name}, {e}")
                continue
                
            if not isinstance(func_args, dict):
                self._dbg(f"[FUNCTION] ì˜ëª»ëœ ì¸ì í˜•ì‹: {func_name}")
                continue
            
            # í•¨ìˆ˜ ë ˆí¼ëŸ°ìŠ¤ ì°¾ê¸°
            func = self.available_functions.get(func_name)
            if not func:
                self._dbg(f"[FUNCTION] ë¯¸ë“±ë¡ í•¨ìˆ˜: {func_name}")
                continue
            
            # í•¨ìˆ˜ ì‹¤í–‰
            try:
                # ì•ˆì „ ê¸°ë³¸ê°’ ë³´ê°•
                if func_name == "search_internet":
                    func_args.setdefault("chat_context", self.context[:])
                    func_args.setdefault("token_counter", self.token_counter)  # í† í° ì¹´ìš´í„° ì „ë‹¬
                elif func_name == "get_halla_cafeteria_menu":
                    func_args.setdefault("date", "ì˜¤ëŠ˜")
                    # mealì€ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´ ë¼ë‹ˆ ë°˜í™˜
                elif func_name == "get_shuttle_bus_info":
                    func_args.setdefault("chat_context", self.context[:])
                    func_args.setdefault("token_counter", self.token_counter)

                sanitized_args = self._sanitize_function_arguments(func_args)
                
                # async í•¨ìˆ˜ì¸ ê²½ìš° await ì‚¬ìš© (ìºì‹œ ì¡°íšŒ)
                try:
                    if self._async_function_flags.get(func_name, False):
                        output = await func(**func_args)
                    else:
                        output = func(**func_args)
                except asyncio.CancelledError:
                    self._dbg(f"[FUNCTION] {func_name} cancelled")
                    raise
                except Exception as e:
                    self._dbg(f"[FUNCTION] {func_name} execution failed: {e}")
                    output = f"âŒ í•¨ìˆ˜ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"
                
                # í•¨ìˆ˜ í˜¸ì¶œ í† í° ê³„ì‚°
                self.token_counter.count_function_call(func_name, sanitized_args, str(output))
                
                # ë©”íƒ€ë°ì´í„° ìƒì„± (reasoning í¬í•¨)
                func_results.append(FunctionCallMetadata(
                    name=func_name,
                    arguments=sanitized_args,
                    output=str(output),
                    call_id=call_id,
                    is_fallback=False,
                    reasoning=reasoning  # LLMì´ ìƒì„±í•œ í•¨ìˆ˜ ì„ íƒ ê·¼ê±°
                ))
                
            except Exception as exc:
                self._dbg(f"[FUNCTION] {func_name} ì‹¤í–‰ ì‹¤íŒ¨: {exc}")
                # ì‹¤íŒ¨í•´ë„ ë©”íƒ€ë°ì´í„°ëŠ” ê¸°ë¡ (ì—ëŸ¬ ë©”ì‹œì§€ í¬í•¨)
                sanitized_args = self._sanitize_function_arguments(func_args)
                func_results.append(FunctionCallMetadata(
                    name=func_name,
                    arguments=sanitized_args,
                    output=f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {str(exc)}",
                    call_id=call_id,
                    is_fallback=False,
                    reasoning=reasoning
                ))
        
        # 1.5) Reasoningì—ì„œ ì„ íƒëœ ë„êµ¬ê°€ ìˆì§€ë§Œ function_callì´ ì—†ëŠ” ê²½ìš° ê°•ì œ ì‹¤í–‰
        if not function_calls_found and selected_tools_from_reasoning:
            self._dbg(f"[FUNCTION] Reasoning ê¸°ë°˜ ê°•ì œ ì‹¤í–‰: {selected_tools_from_reasoning}")
            
            for tool_name in selected_tools_from_reasoning:
                # ì´ë¯¸ ì‹¤í–‰ëœ í•¨ìˆ˜ëŠ” ìŠ¤í‚µ
                if any(meta.name == tool_name for meta in func_results):
                    continue
                
                func = self.available_functions.get(tool_name)
                if not func:
                    self._dbg(f"[FUNCTION] ë¯¸ë“±ë¡ í•¨ìˆ˜ (reasoning ê°•ì œ): {tool_name}")
                    continue
                
                try:
                    # ê¸°ë³¸ ì¸ì ì„¤ì •
                    func_args = {}
                    if tool_name == "search_internet":
                        func_args = {
                            "user_input": message,
                            "chat_context": self.context[:],
                            "token_counter": self.token_counter
                        }
                    elif tool_name == "get_halla_cafeteria_menu":
                        # ë©”ì‹œì§€ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                        lowered_msg = message.lower()
                        date_pref = "ì˜¤ëŠ˜"
                        if "ëª¨ë ˆ" in lowered_msg:
                            date_pref = "ëª¨ë ˆ"
                        elif "ë‚´ì¼" in lowered_msg:
                            date_pref = "ë‚´ì¼"
                        elif "ì–´ì œ" in lowered_msg:
                            date_pref = "ì–´ì œ"
                        else:
                            import re
                            m = re.search(r"(\d{4}[./-]\d{1,2}[./-]\d{1,2})", message)
                            if m:
                                date_pref = m.group(1)
                        func_args = {"date": date_pref, "meal": None}
                    
                    self._dbg(f"[FUNCTION] Reasoning ê°•ì œ ì‹¤í–‰ ì¤‘: {tool_name}")
                    
                    # async í•¨ìˆ˜ì¸ ê²½ìš° await ì‚¬ìš© (ìºì‹œ ì¡°íšŒ)
                    try:
                        if self._async_function_flags.get(tool_name, False):
                            output = await func(**func_args)
                        else:
                            output = func(**func_args)
                    except asyncio.CancelledError:
                        self._dbg(f"[FUNCTION] {tool_name} cancelled during reasoning")
                        raise
                    except Exception as e:
                        self._dbg(f"[FUNCTION] {tool_name} execution failed: {e}")
                        output = f"âŒ í•¨ìˆ˜ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"
                    
                    # í•¨ìˆ˜ í˜¸ì¶œ í† í° ê³„ì‚°
                    sanitized_args = self._sanitize_function_arguments(func_args)
                    self.token_counter.count_function_call(tool_name, sanitized_args, str(output))
                    
                    # ë©”íƒ€ë°ì´í„° ìƒì„±
                    func_results.append(FunctionCallMetadata(
                        name=tool_name,
                        arguments=sanitized_args,
                        output=str(output),
                        call_id=f"reasoning_forced_{tool_name}",
                        is_fallback=True,  # reasoning ê¸°ë°˜ ê°•ì œ ì‹¤í–‰
                        reasoning=f"Reasoningì—ì„œ ì„ íƒë¨: {reasoning}"
                    ))
                    
                    self._dbg(f"[FUNCTION] Reasoning ê°•ì œ ì‹¤í–‰ ì„±ê³µ: {tool_name}")
                    
                except Exception as exc:
                    self._dbg(f"[FUNCTION] Reasoning ê°•ì œ ì‹¤í–‰ ì‹¤íŒ¨: {tool_name}, {exc}")
                    func_results.append(FunctionCallMetadata(
                        name=tool_name,
                        arguments=func_args if 'func_args' in locals() else {},
                        output=f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {str(exc)}",
                        call_id=f"reasoning_forced_{tool_name}_error",
                        is_fallback=True,
                        reasoning=f"Reasoningì—ì„œ ì„ íƒë¨ (ì‹¤í–‰ ì‹¤íŒ¨): {reasoning}"
                    ))
        
        # 2) í•™ì‹ ë³´ê°• Fallback
        lowered = message.lower()
        cafeteria_keywords = any(k in lowered for k in ["í•™ì‹", "ì‹ë‹¨", "ì ì‹¬", "ì €ë…", "ë©”ë‰´", "ì¡°ì‹", "ì„ì‹", "ì•„ì¹¨", "ì˜¤ëŠ˜ ë©”ë‰´", "ë°¥ ë­"])
        already_called_cafeteria = any(meta.name == "get_halla_cafeteria_menu" for meta in func_results)
        
        if cafeteria_keywords and not already_called_cafeteria:
            try:
                self._dbg("[FUNCTION] Cafeteria fallback engaged")
                
                # ë¼ë‹ˆ ì¶”ì¶œ
                meal_pref = None
                if any(x in lowered for x in ["ì¡°ì‹", "ì•„ì¹¨"]):
                    meal_pref = "ì¡°ì‹"
                elif any(x in lowered for x in ["ì„ì‹", "ì €ë…"]):
                    meal_pref = "ì„ì‹"
                elif "ì ì‹¬" in lowered or "ì¤‘ì‹" in lowered:
                    meal_pref = "ì¤‘ì‹"

                # ë‚ ì§œëŠ” ê¸°ë³¸ê°’ ì‚¬ìš© (Function callingì—ì„œ ì´ë¯¸ ì •ê·œí™”ë˜ì–´ ë„˜ì–´ì˜´)
                caf_args = {"date": "ì˜¤ëŠ˜", "meal": meal_pref}
                get_cafeteria_fn = self.available_functions.get("get_halla_cafeteria_menu")
                
                if not get_cafeteria_fn:
                    raise RuntimeError("get_halla_cafeteria_menu not registered")
                
                caf_out = get_cafeteria_fn(**caf_args)
                
                # í•¨ìˆ˜ í˜¸ì¶œ í† í° ê³„ì‚°
                sanitized_caf_args = self._sanitize_function_arguments(caf_args)
                self.token_counter.count_function_call("get_halla_cafeteria_menu", sanitized_caf_args, str(caf_out))
                
                # Fallback ë©”íƒ€ë°ì´í„° ì¶”ê°€ (í‚¤ì›Œë“œ ê¸°ë°˜ í˜¸ì¶œì€ reasoning ì—†ìŒ)
                func_results.append(FunctionCallMetadata(
                    name="get_halla_cafeteria_menu",
                    arguments=sanitized_caf_args,
                    output=str(caf_out),
                    call_id="cafeteria_auto",
                    is_fallback=True,
                    reasoning="í‚¤ì›Œë“œ ê¸°ë°˜ í•™ì‹ ë©”ë‰´ ìë™ í˜¸ì¶œ"
                ))
                
            except Exception as e:
                self._dbg(f"[FUNCTION] í•™ì‹ fallback ì‹¤íŒ¨: {e}")
        
        return reasoning, func_results

    async def _stream_openai_response(
        self,
        context: List[Dict[str, str]]
    ):
        """OpenAI Responses API ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ

        JSON Lines í˜•ì‹ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ë¥¼ yieldí•©ë‹ˆë‹¤.

        Args:
            context: OpenAI APIì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸

        Yields:
            Dict: ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸
                - {"type": "delta", "content": "..."}: í…ìŠ¤íŠ¸ ì²­í¬
                - {"type": "completed", "text": "..."}: ì™„ë£Œëœ ì „ì²´ í…ìŠ¤íŠ¸
        """
        completed_text = ""

        try:
            # Note: Responses APIëŠ” stream_optionsë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
            # streaming output tokensëŠ” tiktokenìœ¼ë¡œ ê³„ì‚°
            response_stream = client.responses.create(
                model=self.model,
                input=context,
                top_p=1,
                stream=True,
                text={"format": {"type": "text"}}
            )

            for event in response_stream:
                if event.type == "response.output_text.delta":
                    # ë¸íƒ€ ì´ë²¤íŠ¸ - ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ì²­í¬
                    yield {
                        "type": "delta",
                        "content": event.delta
                    }
                    completed_text += event.delta

                    # ì¶œë ¥ í† í° ê³„ì‚° (ì„ì‹œ, response.doneì—ì„œ API usageë¡œ êµì²´)
                    self.token_counter.count_output_delta(event.delta)

                elif event.type == "response.output_item.done":
                    # ì¶œë ¥ ì•„ì´í…œ ì™„ë£Œ - ì „ì²´ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
                    item = event.item
                    if item.type == "message" and item.role == "assistant":
                        for part in item.content:
                            if getattr(part, "type", None) == "output_text":
                                completed_text = part.text

                elif event.type == "response.failed":
                    # ì‹¤íŒ¨ ì´ë²¤íŠ¸
                    yield {
                        "type": "error",
                        "message": "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"
                    }
                    return

                elif event.type == "response.completed":
                    # ì™„ë£Œ ì´ë²¤íŠ¸ - API usage ì •ë³´ ì¶”ì¶œ
                    if hasattr(event, 'usage') and event.usage:
                        usage_data = {
                            "input_tokens": getattr(event.usage, "input_tokens", 0),
                            "output_tokens": getattr(event.usage, "output_tokens", 0),
                            "total_tokens": getattr(event.usage, "total_tokens", 0),
                            "reasoning_tokens": getattr(event.usage.output_tokens_details, 'reasoning_tokens', 0) if hasattr(event.usage, 'output_tokens_details') else 0,
                        }

                        # TokenCounter ì—…ë°ì´íŠ¸ (tiktoken ì¶”ì •ê°’ì„ API ì‹¤ì œê°’ìœ¼ë¡œ êµì²´)
                        self.token_counter.update_from_api_usage(
                            usage=usage_data,
                            role="streaming",
                            model=self.model,
                            category="input",
                            replace=True
                        )

                        self._dbg(f"[STREAM] API usage ë°˜ì˜: input={usage_data['input_tokens']}, "
                                  f"output={usage_data['output_tokens']}, "
                                  f"reasoning={usage_data['reasoning_tokens']}")

            # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ë²„í¼ í”ŒëŸ¬ì‹œ (ë‚¨ì€ ë¸íƒ€ ì²˜ë¦¬)
            self._dbg(f"[STREAM] ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ, ë¸íƒ€ ë²„í¼ í”ŒëŸ¬ì‹œ")
            self.token_counter.flush_delta_buffer(role="streaming")

            # ì™„ë£Œ ì´ë²¤íŠ¸
            yield {
                "type": "completed",
                "text": completed_text
            }

        except Exception as e:
            self._dbg(f"[STREAM] OpenAI ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            yield {
                "type": "error",
                "message": f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}"
            }

    async def stream_chat(
        self, 
        message: str,
        message_history: Optional[List[Dict[str, str]]] = None,
        language: str = "KOR"
    ) -> AsyncGenerator[str, None]:
        """ì±—ë´‡ ìŠ¤íŠ¸ë¦¬ë° í†µí•© ë©”ì„œë“œ

        JSON Lines í˜•ì‹ìœ¼ë¡œ ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.

        ì²˜ë¦¬ íë¦„:
        1. ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
        2. RAG ê²€ìƒ‰ + í•¨ìˆ˜ í˜¸ì¶œ ë³‘ë ¬ ì‹¤í–‰ (asyncio.gather)
        3. RAG ìš”ì•½ (RAG ê²°ê³¼ê°€ ìˆì„ ê²½ìš°ë§Œ)
        4. í•¨ìˆ˜ í˜¸ì¶œ ë©”íƒ€ë°ì´í„° ì„¤ì •
        5. ìµœì¢… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ì–¸ì–´ ì§€ì¹¨ í¬í•¨)
        6. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        6.5. ì¶œì²˜ ì •ë³´ ìŠ¤íŠ¸ë¦¬ë°
        7. ë©”íƒ€ë°ì´í„° ì „ì†¡
        8. ì™„ë£Œ ì‹ í˜¸
        9. ì‘ë‹µ ì €ì¥

        Args:
            message: í˜„ì¬ ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€
            message_history: í”„ë¡ íŠ¸ê°€ ì „ë‹¬í•œ ê¸°ì¡´ ëŒ€í™” íˆìŠ¤í† ë¦¬ (optional)
            language: ì‘ë‹µ ì–¸ì–´ (KOR, ENG, VI, JPN, CHN, UZB, MNG, IDN)

        Yields:
            JSON Lines í˜•ì‹ì˜ ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸
        """
        user_input = message

        self._load_message_history(message_history)
        self._dbg(f"[STREAM_CHAT] ì‹œì‘ - ë©”ì‹œì§€: {user_input[:50]}..., ì–¸ì–´: {language}, íˆìŠ¤í† ë¦¬ ê¸¸ì´: {len(message_history or [])}")
        
        # === 1ë‹¨ê³„: ì´ˆê¸°í™” ===
        self.add_user_message_in_context(user_input)
        metadata = ChatMetadata()
        self.token_counter.reset()  # í† í° ì¹´ìš´í„° ì´ˆê¸°í™”
        self._dbg("[STREAM_CHAT] 1ë‹¨ê³„: ë©”ì‹œì§€ ì¶”ê°€ ì™„ë£Œ")

        # === 2ë‹¨ê³„: RAG ê²€ìƒ‰ + í•¨ìˆ˜ í˜¸ì¶œ ë³‘ë ¬ ì‹¤í–‰ ===
        self._dbg("[STREAM_CHAT] 2ë‹¨ê³„: RAG ê²€ìƒ‰ê³¼ í•¨ìˆ˜ í˜¸ì¶œ ë³‘ë ¬ ì‹œì‘...")
        import asyncio
        
        # ë³‘ë ¬ ì‹¤í–‰: RAG ê²€ìƒ‰ê³¼ í•¨ìˆ˜ í˜¸ì¶œì„ ë™ì‹œì— ì‹¤í–‰
        rag_result, (func_reasoning, func_results) = await asyncio.gather(
            self.rag_service.retrieve_context(user_input),
            self._analyze_and_execute_functions(user_input)
        )
        
        self._dbg(f"[STREAM_CHAT] ë³‘ë ¬ ì‹¤í–‰ ì™„ë£Œ - RAG: {len(rag_result.hits)}ê°œ, í•¨ìˆ˜: {len(func_results)}ê°œ")
        
        # === 3ë‹¨ê³„: RAG ìš”ì•½ (RAG ê²°ê³¼ê°€ ìˆì„ ê²½ìš°ë§Œ) ===
        condensed_rag = None
        
        if rag_result.merged_documents_text:
            self._dbg(f"[STREAM_CHAT] RAG ê²€ìƒ‰ ì™„ë£Œ - ì›ë³¸ ê¸¸ì´: {len(rag_result.merged_documents_text)}ì")
            
            # === RAG ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë””ë²„ê·¸ ì¶œë ¥ ===
            self._dbg("=" * 80)
            self._dbg("[RAG ë””ë²„ê·¸] ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ì •ë³´:")
            self._dbg(f"  - ê·œì • ì—¬ë¶€: {rag_result.is_regulation}")
            self._dbg(f"  - íŒë‹¨ ì´ìœ : {rag_result.gate_reason or 'N/A'}")
            self._dbg(f"  - ê²€ìƒ‰ ì†ŒìŠ¤: {rag_result.context_source}")
            self._dbg(f"  - ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(rag_result.hits)}")
            self._dbg(f"  - MongoDB ë¬¸ì„œ: {rag_result.document_count}ê°œ")
            self._dbg(f"  - í”„ë¦¬ë·° ë¬¸ì„œ: {rag_result.preview_count}ê°œ")
            self._dbg(f"  - ì²­í¬ ID ìˆ˜: {len(rag_result.chunk_ids)}ê°œ")
            #ê²€ìƒ‰ë¬¸ì„œ id ìƒ˜í”Œ ì¶œë ¥ (ìµœëŒ€ 5ê°œ) 5ê°œ ì´ˆê³¼ ì‹œ ìƒëµí‘œì‹œ
            if rag_result.chunk_ids:
                chunk_ids_str = ", ".join(rag_result.chunk_ids[:5])
                if len(rag_result.chunk_ids) > 5:
                    chunk_ids_str += f" ... (ì™¸ {len(rag_result.chunk_ids) - 5}ê°œ)"
                self._dbg(f"  - ì²­í¬ ID ìƒ˜í”Œ: [{chunk_ids_str}]")
            
            # ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ ìƒ˜í”Œ ì¶œë ¥ (ì²˜ìŒ 500ì)
           # context_sample = rag_result.merged_documents_text[:500]
            #if len(rag_result.merged_documents_text) > 500:
            #    context_sample += "..."
            #self._dbg(f"  - ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ ìƒ˜í”Œ:\n{context_sample}")
            #self._dbg("=" * 80)
            
            self._dbg("[STREAM_CHAT] 3ë‹¨ê³„: RAG ìš”ì•½ ì‹œì‘...")
            condensed_rag = await self._condense_rag_context(
                user_input, rag_result.merged_documents_text
            )
            self._dbg(f"[STREAM_CHAT] RAG ìš”ì•½ ì™„ë£Œ - ìš”ì•½ ê¸¸ì´: {len(condensed_rag)}ì")
            # RAG ë©”íƒ€ë°ì´í„° ì„¤ì •
            
            metadata.rag = RagMetadata(
                is_regulation=rag_result.is_regulation,
                gate_reason=rag_result.gate_reason or "",
                context_source=rag_result.context_source,
                hits_count=len(rag_result.hits),
                document_count=rag_result.document_count,
                preview_count=rag_result.preview_count,
                chunk_ids=list(rag_result.chunk_ids),
                source_documents=rag_result.source_documents,  # ì¶œì²˜ ë¬¸ì„œ ì •ë³´ ì¶”ê°€
                raw_context=rag_result.merged_documents_text,  # ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
                condensed_context=condensed_rag,
            )
        else:
            self._dbg("[STREAM_CHAT] RAG ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            # RAG Gate íŒë‹¨ ê²°ê³¼ëŠ” í•­ìƒ metadataì— í¬í•¨ (ë””ë²„ê¹…ìš©)
            metadata.rag = RagMetadata(
                is_regulation=rag_result.is_regulation,
                gate_reason=rag_result.gate_reason or "RAG Gate íŒë‹¨: ë¬¸ì„œ ì—†ìŒ",
                context_source=rag_result.context_source,
                hits_count=len(rag_result.hits),
                document_count=rag_result.document_count,
                preview_count=rag_result.preview_count,
                chunk_ids=list(rag_result.chunk_ids),
                source_documents=[],
                raw_context=None,
                condensed_context=None,
            )
        
        # === 4ë‹¨ê³„: í•¨ìˆ˜ í˜¸ì¶œ ë©”íƒ€ë°ì´í„° ì„¤ì • (ì´ë¯¸ 2ë‹¨ê³„ì—ì„œ ì‹¤í–‰ ì™„ë£Œ) ===
        metadata.functions = func_results
        self._dbg(f"[STREAM_CHAT] 4ë‹¨ê³„: í•¨ìˆ˜ ë©”íƒ€ë°ì´í„° ì„¤ì • ì™„ë£Œ - {len(func_results)}ê°œ")
        
        # í•¨ìˆ˜ ì„ íƒ ì¶”ë¡  ë©”íƒ€ë°ì´í„° ì„¤ì •
        if func_reasoning and func_results:
            # ì‹¤ì œë¡œ í•¨ìˆ˜ê°€ í˜¸ì¶œëœ ê²½ìš°ì—ë§Œ ì¶”ë¡  ë©”íƒ€ë°ì´í„° ì¶”ê°€
            selected_tool_names = [f.name for f in func_results]
            metadata.tool_reasoning = ToolReasoningMetadata(
                reasoning=func_reasoning,
                selected_tools=selected_tool_names
            )
            self._dbg(f"[STREAM_CHAT] í•¨ìˆ˜ ì„ íƒ ì¶”ë¡ : {func_reasoning}")
        
        # ì›¹ê²€ìƒ‰ ìƒíƒœ ì„¤ì •
        web_search_used = any(meta.name == "search_internet" for meta in func_results)
        if web_search_used:
            # ì›¹ê²€ìƒ‰ ê²°ê³¼ ì˜¤ë¥˜ í™•ì¸
            web_meta = next((m for m in func_results if m.name == "search_internet"), None)
            if web_meta and any(k in web_meta.output.lower() for k in ["ì˜¤ë¥˜", "error", "âŒ"]):
                metadata.web_search_status = "empty-or-error"
            else:
                metadata.web_search_status = "ok"
        else:
            metadata.web_search_status = "not-run"
        
        # === 5ë‹¨ê³„: ìµœì¢… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ì–¸ì–´ ì§€ì¹¨ í¬í•¨) ===
        final_context = self._build_final_context(
            message=user_input,
            condensed_rag=condensed_rag,
            func_results=func_results,
            language=language,
        )
        
        # ì…ë ¥ í† í° ê³„ì‚° (OpenAI API í˜•ì‹ ì˜¤ë²„í—¤ë“œ í¬í•¨, ì—­í•  ì¶”ì  í¬í•¨)
        self.token_counter.count_openai_streaming_tokens(final_context, role="streaming")
        
        # === 6ë‹¨ê³„: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ===
        completed_text = ""
        async for chunk in self._stream_openai_response(final_context):
            if chunk["type"] == "delta":
                yield json.dumps(chunk, ensure_ascii=False) + "\n"
            elif chunk["type"] == "completed":
                completed_text = chunk["text"]
            elif chunk["type"] == "error":
                yield json.dumps(chunk, ensure_ascii=False) + "\n"
                return
        
        # === 6.5ë‹¨ê³„: ì¶œì²˜ ì •ë³´ ìŠ¤íŠ¸ë¦¬ë° ===
        self._dbg("[STREAM_CHAT] 6.5ë‹¨ê³„: ì¶œì²˜ ì •ë³´ ìŠ¤íŠ¸ë¦¬ë°")
        
        # (1) ì›¹ ê²€ìƒ‰ ë§í¬ í‘œì‹œ
        web_links = self._extract_web_links(func_results)
        if web_links:
            self._dbg(f"[STREAM_CHAT] ì›¹ ë§í¬ {len(web_links)}ê°œ í‘œì‹œ")
            yield json.dumps({"type": "delta", "content": "\n\nğŸ“ ì°¸ê³  ë§í¬:\n"}, ensure_ascii=False) + "\n"
            for link in web_links:
                yield json.dumps({"type": "delta", "content": f"  - {link}\n"}, ensure_ascii=False) + "\n"
        
        # (2) RAG ë¬¸ì„œ ì¶œì²˜ í‘œì‹œ
        if metadata.rag and metadata.rag.source_documents:
            sources_text = self._format_rag_sources(metadata.rag.source_documents)
            if sources_text:
                self._dbg(f"[STREAM_CHAT] RAG ì¶œì²˜ {len(metadata.rag.source_documents)}ê°œ í‘œì‹œ")
                yield json.dumps({"type": "delta", "content": "\n\nğŸ“š ì°¸ê³  ë¬¸ì„œ:\n"}, ensure_ascii=False) + "\n"
                yield json.dumps({"type": "delta", "content": sources_text + "\n"}, ensure_ascii=False) + "\n"
        
        # === 7ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ì „ì†¡ ===
        self._dbg("[STREAM_CHAT] 7ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ì „ì†¡")
        
        # í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ê³„ì‚°
        token_usage = self.token_counter.get_total()

        # âœ… Roleë³„ ëª¨ë¸ì„ ë°˜ì˜í•œ ì •í™•í•œ ë¹„ìš© ê³„ì‚°
        role_usage_list = self.token_counter.get_role_usage_for_cost_calc()
        if role_usage_list:
            # Roleë³„ ëª¨ë¸ ì‚¬ìš© -> ë°°ì¹˜ ë¹„ìš© ê³„ì‚°
            cost_data = self.cost_calculator.calculate_batch(role_usage_list)
            input_cost_usd = sum(m["input_cost_usd"] for m in cost_data["by_model"].values())
            output_cost_usd = sum(m["output_cost_usd"] for m in cost_data["by_model"].values())
            total_cost_usd = cost_data["total_cost_usd"]
        else:
            # í´ë°±: ê¸°ì¡´ ë°©ì‹ (streaming ëª¨ë¸ë¡œ í†µí•© ê³„ì‚°)
            cost_data_fallback = self.cost_calculator.calculate(
                token_usage=token_usage,
                model=self.model
            )
            input_cost_usd = cost_data_fallback["input_cost_usd"]
            output_cost_usd = cost_data_fallback["output_cost_usd"]
            total_cost_usd = cost_data_fallback["total_cost_usd"]

        # TokenUsageMetadata ìƒì„± ë° ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
        # í˜„ì¬ í™œì„± í”„ë¦¬ì…‹ ê°€ì ¸ì˜¤ê¸°
        llm_manager = get_llm_manager()
        active_preset = llm_manager.get_active_preset()

        metadata.token_usage = TokenUsageMetadata(
            input_tokens=token_usage["input_tokens"],
            output_tokens=token_usage["output_tokens"],
            function_tokens=token_usage["function_tokens"],
            rag_tokens=token_usage["rag_tokens"],
            reasoning_tokens=token_usage.get("reasoning_tokens", 0),
            total_tokens=token_usage["total_tokens"],
            input_cost_usd=input_cost_usd,
            output_cost_usd=output_cost_usd,
            total_cost_usd=total_cost_usd,
            currency="USD",
            model=self.model,
            preset=active_preset,  # í˜„ì¬ í”„ë¦¬ì…‹ ì¶”ê°€
            tracking_mode=self.token_counter.get_tracking_mode(),  # í† í° ì¶”ì  ëª¨ë“œ
            role_breakdown=self.token_counter.get_role_breakdown()  # ì—­í• ë³„ í† í° ìƒì„¸ ì¶”ê°€
        )
        
        yield json.dumps({
            "type": "metadata",
            "data": metadata.to_dict()
        }, ensure_ascii=False) + "\n"
        
        # === 8ë‹¨ê³„: ì™„ë£Œ ì‹ í˜¸ ===
        self._dbg("[STREAM_CHAT] 8ë‹¨ê³„: ì™„ë£Œ ì‹ í˜¸ ì „ì†¡")
        yield json.dumps({"type": "done"}, ensure_ascii=False) + "\n"
        
        # === 9ë‹¨ê³„: ì‘ë‹µ ì €ì¥ ===
        # í”„ë¡ íŠ¸ì—”ë“œê°€ íˆìŠ¤í† ë¦¬ë¥¼ ê´€ë¦¬í•˜ë¯€ë¡œ ë‚´ë¶€ ì»¨í…ìŠ¤íŠ¸ ëˆ„ì ì€ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
        # self.add_response_stream(completed_text)
        self._dbg(f"[STREAM_CHAT] 9ë‹¨ê³„: ì‘ë‹µ ì €ì¥ ì™„ë£Œ - ê¸¸ì´: {len(completed_text)}ì")


        self._dbg("[STREAM_CHAT] ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ!")
