import json
import requests
from pprint import pprint
import re
import time
from datetime import datetime, timedelta
from typing import Optional
from bs4 import BeautifulSoup
import os
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv
from dataclasses import dataclass

# LLM Manager import
try:
    from app.ai.llm import get_provider
except ImportError:
    # ìƒëŒ€ ê²½ë¡œë¡œ ì‹œë„
    from ..llm import get_provider

# ìˆœí™˜ ì°¸ì¡° ë°©ì§€: config ëŒ€ì‹  ì§ì ‘ ìƒì„±
_BASE_DIR = Path(__file__).resolve().parent.parent.parent  # app/
_DOTENV_PATH = _BASE_DIR / "apikey.env"
load_dotenv(_DOTENV_PATH)

@dataclass(frozen=True)
class Model: 
    basic: str = "gpt-3.5-turbo-1106"
    advanced: str = "gpt-4.1"
    o3_mini: str = "o3-mini"
    o1: str = "o1"

model = Model()
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key, max_retries=1)

def makeup_response(message, finish_reason="ERROR"):
    '''api ì‘ë‹µí˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì„œ
       ê°œë°œìê°€ ì„ì˜ë¡œ ìƒì„±í•œ ë©”ì„¸ì§€ë¥¼
       ê¸°ì¡´ ì¶œë ¥ í•¨ìˆ˜ë¡œ ì¶œë ¥í•˜ëŠ” ìš©ë„ì¸ í•¨ìˆ˜'''
    return {
        "choices": [
            {
                "finish_reason": finish_reason,
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": message
                }                   
            }
        ],
        "usage": {"total_tokens": 0},
    }
    
tools = [
        
            {
            "type": "function",
            "name": "search_internet",
            "description": "Searches the internet based on user input and retrieves relevant information",
            "parameters": {
                "type": "object",
                "required": [
                "user_input"
                ],
                "properties": {
                "user_input": {
                    "type": "string",
                    "description": "User's search query input(conversation context will be automatically added)"
                }
                },
                "additionalProperties": False
            }
            },
            {
            "type": "function",
            "name": "get_halla_cafeteria_menu",
            "description": "ì›ì£¼ í•œë¼ëŒ€í•™êµ í•™ìƒì‹ë‹¹ì˜ ë©”ë‰´ë¥¼ ê¶ê¸ˆí•´ í•˜ë©´ ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”. ì£¼ê°„ ì‹ë‹¨ í˜ì´ì§€ì—ì„œ íŠ¹ì • ë‚ ì§œ/ë¼ë‹ˆì˜ ë©”ë‰´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.",
            "parameters": {
                "type": "object",
                "required": ["date"],
                "properties": {
                    "date": {
                        "type": "string",
                        "description": "ì¡°íšŒí•  ë‚ ì§œ. í˜•ì‹ YYYY-MM-DD ë˜ëŠ” YYYY.MM.DD. 'ì˜¤ëŠ˜', 'ë‚´ì¼' í—ˆìš©. ê¸°ë³¸ê°’ì€ 'ì˜¤ëŠ˜'ì…ë‹ˆë‹¤.",
                    },
                    "meal": {
                        "type": "string",
                        "enum": ["ì¡°ì‹", "ì¤‘ì‹", "ì„ì‹"],
                        "description": "ì¡°íšŒí•  ë¼ë‹ˆ. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´ ë¼ë‹ˆë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.",
                    }
                },
                "additionalProperties": False
            }
            },
      
       
      
    ]

# --- ê³µì§€ ì¹´í…Œê³ ë¦¬ LLM ë¶„ë¥˜ê¸° ---
async def _classify_notice_category_llm(user_input: str, context_info: str | None = None, token_counter=None) -> str | None:
    """ì‚¬ìš©ì ì…ë ¥ì´ ì–´ë–¤ ê³µì§€ì‚¬í•­ ì¹´í…Œê³ ë¦¬ì¸ì§€ LLMìœ¼ë¡œ ë¶„ë¥˜í•˜ì—¬ ì¹´í…Œê³ ë¦¬ ë¬¸ìì—´ì„ ë°˜í™˜.
    ë°˜í™˜ ê°€ëŠ¥ ê°’: "í•™ì‚¬ê³µì§€", "ë¹„êµê³¼ê³µì§€", "ì¥í•™ê³µì§€", "ì¼ë°˜ê³µì§€", "í•´ë‹¹ì—†ìŒ". ì¸ì‹ ì‹¤íŒ¨ ì‹œ None.
    """
    try:
        allowed = ["í•™ì‚¬ê³µì§€", "ë¹„êµê³¼ê³µì§€", "ì¥í•™ê³µì§€", "ì¼ë°˜ê³µì§€", "í•´ë‹¹ì—†ìŒ"]
        prompt = (
            "ë‹¤ìŒ ì‚¬ìš©ìì˜ ìš”ì²­ì´ í•œë¼ëŒ€í•™êµ 'ê³µì§€' ì¤‘ ì–´ë–¤ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ”ì§€ í•˜ë‚˜ë§Œ ì„ íƒí•´ ë‹µí•˜ì„¸ìš”.\n"
            "ì¹´í…Œê³ ë¦¬: í•™ì‚¬ê³µì§€ | ë¹„êµê³¼ê³µì§€ | ì¥í•™ê³µì§€ | ì¼ë°˜ê³µì§€ | í•´ë‹¹ì—†ìŒ\n"
            "ê·œì¹™:\n"
            "- ì •í™•íˆ ìœ„ì˜ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ë§, ì„¤ëª…, ë”°ì˜´í‘œ ì—†ì´.\n"
            f"ì‚¬ìš©ì ì…ë ¥: {user_input}\n"
            f"ëŒ€í™” ë¬¸ë§¥: {context_info or '(ì—†ìŒ)'}\n\n"
            "ì •ë‹µ:"
        )

        # LLM Managerë¥¼ í†µí•´ Provider ì„ íƒ (êµì²´ ê°€ëŠ¥)
        provider = get_provider("category")
        messages = [{
            "role": "user",
            "content": [{"type": "input_text", "text": prompt}],
        }]
        raw, usage = await provider.simple_completion(messages)
        raw = raw.strip()

        # âœ… API usage ê¸°ë°˜ í† í° ê³„ì‚°
        if token_counter and usage:
            token_counter.update_from_api_usage(
                usage=usage,
                role="category",
                model=provider.get_model_name(),
                category="function"
            )
        
        print("ê³µì§€ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ê¸° ì›ë¬¸:", raw)
        # ì •ê·œí™” ë° ì„ íƒ
        text_norm = raw.replace(" ", "").replace("\n", "")
        for a in allowed:
            if a in text_norm:
                return a
        return None
    except Exception as e:
        print(f"[_classify_notice_category_llm] Error: {e}")
        return None

# --- ê·œì¹™ ê¸°ë°˜ ì‚¬ì´íŠ¸ ì„ í˜¸ ë¼ìš°íŒ… ---
async def _prefer_halla_site_query(user_input: str, context_info: str | None = None, token_counter=None) -> str | None:
    """íŠ¹ì • ìš”êµ¬ì‚¬í•­ì¼ ë•Œ í•œë¼ëŒ€ íŠ¹ì • í˜ì´ì§€ë¥¼ ìš°ì„  íƒìƒ‰í•˜ë„ë¡ ê²€ìƒ‰ì–´ë¥¼ êµ¬ì„±.
    ë§¤ì¹­ë˜ë©´ URLê³¼ site í•„í„°ë¥¼ í¬í•¨í•œ ì¿¼ë¦¬ë¥¼ ë°˜í™˜, ì—†ìœ¼ë©´ None.
    """
    base = (context_info or "")
    text = f"{user_input}\n{base}".lower()

    # ë©”ë‰´/í•™ì‹ ë¼ìš°íŒ…
    menu_keywords = ["í•™ì‹", "ì‹ë‹¨", "ë©”ë‰´", "ì ì‹¬", "ì €ë…", "ì˜¤ëŠ˜ ë©”ë‰´"]
    if any(k in text for k in menu_keywords):
        url = "https://www.halla.ac.kr/kr/211/subview.do"
        return f"site:halla.ac.kr {url} {user_input}"

    # ê³µì§€ ë¼ìš°íŒ…: LLM ë¶„ë¥˜ ê¸°ë°˜ â†’ ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°±
    category = await _classify_notice_category_llm(user_input, context_info, token_counter)
    category_to_url = {
        "í•™ì‚¬ê³µì§€": "https://www.halla.ac.kr/kr/242/subview.do",
        "ë¹„êµê³¼ê³µì§€": "https://www.halla.ac.kr/kr/243/subview.do",
        "ì¥í•™ê³µì§€": "https://www.halla.ac.kr/kr/244/subview.do",
        "ì¼ë°˜ê³µì§€": "https://www.halla.ac.kr/kr/241/subview.do",
    }
    if category and category != "í•´ë‹¹ì—†ìŒ":
        url = category_to_url.get(category)
        if url:
            return f"site:halla.ac.kr {url} {user_input}"

    # í´ë°±: ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­
    if "í•™ì‚¬ê³µì§€" in text:
        url = "https://www.halla.ac.kr/kr/242/subview.do"
        return f"{user_input} site:halla.ac.kr {url} "
    if "ë¹„êµê³¼" in text or "ë¹„êµê³¼ê³µì§€" in text:
        url = "https://www.halla.ac.kr/kr/243/subview.do"
        return f"{user_input} site:halla.ac.kr {url}"
    if "ì¥í•™" in text:
        url = "https://www.halla.ac.kr/kr/244/subview.do"
        return f"{user_input} site:halla.ac.kr {url} "
    if "ì¼ë°˜ê³µì§€" in text or "ê³µì§€" in text:
        url = "https://www.halla.ac.kr/kr/241/subview.do"
        return f"{user_input} site:halla.ac.kr {url}"

    # ë¯¸ë§¤ì¹­ ì‹œ ë¼ìš°íŒ… ì—†ìŒ
    return None

async def search_internet(user_input: str, chat_context=None, token_counter=None) -> str:
    start_ts = time.time()
    print(f"[WEB][START] query='{user_input}' chat_ctx={'Y' if chat_context else 'N'}")
    try:
        if chat_context:
            print("[WEB] context available -> trimming recent messages")
            recent_messages = chat_context[-4:]
            context_info = "\n".join([
                f"{m.get('role','unknown')}: {m.get('content','')}" for m in recent_messages if m.get('role') != 'system'
            ])
        else:
            recent_messages = []
            context_info = ""

        preferred = await _prefer_halla_site_query(user_input, context_info if context_info else None, token_counter)
        
        # LLM ì—ì´ì „íŠ¸ë¡œ ê²€ìƒ‰ì–´ ì¬ì‘ì„± (context_info í¬í•¨)
        rewrite_prompt = (
            f"{user_input}\n\n[ëŒ€í™” ë¬¸ë§¥]: {context_info} ë¥¼ ì°¸ê³ í•´ (ì´ì „ ë¬¸ë§¥ê³¼ ì—°ê²°ëœ í›„ì† ì§ˆë¬¸ì´ë©´ ì—°ê´€ëœ í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨) "
            "ê°„ê²°í•œ ê²€ìƒ‰ì–´ ì¡°í•©ì„ ìƒˆë¡œ ë§Œë“¤ì–´ë¼. ê°€ëŠ¥í•˜ë©´ site:halla.ac.kr ë˜ëŠ” ê´€ë ¨ ê³µì‹ URL í¬í•¨."
        )
        
        # preferredê°€ ìˆìœ¼ë©´ ì¶”ê°€ ì •ë³´ë¡œ í™œìš©
        if preferred:
            rewrite_prompt += f"\n\n[ì¶”ì²œ ì‚¬ì´íŠ¸]: {preferred}"
        
        provider = get_provider("search_rewrite")
        messages = [{"role": "user", "content": [{"type": "input_text", "text": rewrite_prompt}]}]
        search_text, usage = await provider.simple_completion(messages)
        search_text = search_text.strip()

        # âœ… API usage ê¸°ë°˜ í† í° ê³„ì‚°
        if token_counter and usage:
            token_counter.update_from_api_usage(
                usage=usage,
                role="search_rewrite",
                model=provider.get_model_name(),
                category="function"
            )
        print(f"[WEB] final_search_text='{search_text}'")

        context_input = [{
            "role": "user",
            "content": [{"type": "input_text", "text": search_text}]
        }]

        call_ts = time.time()
        response = client.responses.create(
            model=model.advanced,
            input=context_input,
            text={"format": {"type": "text"}},
            reasoning={},
            tools=[{
                "type": "web_search_preview",
                "user_location": {"type": "approximate", "country": "KR"},
                "search_context_size": "medium"
            }],
            tool_choice={"type": "web_search_preview"},
            temperature=1,
            max_output_tokens=2048,
            top_p=1,
            store=True
        )
        print(f"[WEB] openai.responses.create elapsed={time.time()-call_ts:.2f}s total={time.time()-start_ts:.2f}s")

        # âœ… API usage ì¶”ì  (web_search ì—­í• )
        if token_counter:
            if hasattr(response, 'usage') and response.usage:
                # API usage ì •ë³´ ì¶”ì¶œ
                input_tok = getattr(response.usage, "input_tokens", 0)
                output_tok = getattr(response.usage, "output_tokens", 0)

                # reasoning_tokens ì¶”ì¶œ (í•„ìš”ì‹œ)
                reasoning_tok = 0
                if hasattr(response.usage, 'output_tokens_details') and response.usage.output_tokens_details:
                    reasoning_tok = getattr(response.usage.output_tokens_details, 'reasoning_tokens', 0)

                # total_tokens ê³„ì‚°
                total_tok = getattr(response.usage, "total_tokens", input_tok + output_tok)

                usage_data = {
                    "input_tokens": input_tok,
                    "output_tokens": output_tok,
                    "reasoning_tokens": reasoning_tok,
                    "total_tokens": total_tok,
                }

                token_counter.update_from_api_usage(
                    usage=usage_data,
                    role="web_search",
                    model=model.advanced,  # gpt-4.1
                    category="function",
                    replace=False
                )
                print(f"[TokenTrack][web_search] âœ… API usage tracked: input={input_tok}, output={output_tok}, reasoning={reasoning_tok}")
            else:
                print(f"[TokenTrack][web_search] âš ï¸ No API usage available")

        did_call = any(getattr(item, "type", None) == "web_search_call" for item in getattr(response, "output", []))
        print(f"[WEB] search_call_performed={did_call}")

        message = next((item for item in response.output if getattr(item, "type", None) == "message"), None)
        if not message:
            return "âŒ GPT ì‘ë‹µ ë©”ì‹œì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        content_block = next((block for block in message.content if getattr(block, "type", None) == "output_text"), None)
        if not content_block:
            return "âŒ GPT ì‘ë‹µ ë‚´ output_text í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        output_text = getattr(content_block, "text", "").strip()
        annotations = getattr(content_block, "annotations", [])
        citations = []
        for a in annotations:
            if getattr(a, "type", None) == "url_citation":
                title = getattr(a, "title", "ì¶œì²˜")
                url = getattr(a, "url", "")
                if url:
                    citations.append(f"[{title}]({url})")
        result = output_text
        if citations:
            result += "\n\nğŸ“ ì¶œì²˜:\n" + "\n".join(citations)
        print(f"[WEB][END] success total_elapsed={time.time()-start_ts:.2f}s")
        return result + "\n[WEB_METADATA]elapsed={:.2f}s did_call={}".format(time.time()-start_ts, did_call)
    except Exception as e:
        print(f"[WEB][ERROR] {e} total_elapsed={time.time()-start_ts:.2f}s")
        return f"ğŸš¨ ì›¹ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"


def _parse_date_input(date_text: Optional[str]) -> datetime.date:
    today = datetime.now().date()
    if not date_text:
        return today
    s = str(date_text).strip()
    # ìƒëŒ€ ë‚ ì§œ ì§€ì›
    if s in ("ì˜¤ëŠ˜", "today"):
        return today
    if s in ("ë‚´ì¼", "tomorrow"):
        return today + timedelta(days=1)
    if s in ("ì–´ì œ", "yesterday"):
        return today - timedelta(days=1)
    if s in ("ëª¨ë ˆ", "day after tomorrow"):
        return today + timedelta(days=2)
    # Normalize separators and parse flexibly (YYYY.M.D or YYYY.MM.DD)
    s_norm = s.replace("/", ".").replace("-", ".")
    parts = s_norm.split(".")
    if len(parts) == 3 and all(p.isdigit() for p in parts):
        y, m, d = map(int, parts)
        return datetime(year=y, month=m, day=d).date()
    # í•œêµ­ì–´ í‘œê¸°: 9ì›” 7ì¼ (ì—°ë„ ìƒëµ ì‹œ ì˜¬í•´)
    m = re.search(r"(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼", s)
    if m:
        y = today.year
        month = int(m.group(1))
        day = int(m.group(2))
        return datetime(year=y, month=month, day=day).date()
    raise ValueError("ë‚ ì§œ í˜•ì‹ì€ YYYY-MM-DD / YYYY.M.D / 'ì˜¤ëŠ˜/ë‚´ì¼/ì–´ì œ'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")


def get_halla_cafeteria_menu(date: Optional[str] = None, meal: Optional[str] = None) -> str:
    """ì›ì£¼ í•œë¼ëŒ€ í•™ìƒì‹ë‹¹ ì£¼ê°„ ì‹ë‹¨ í˜ì´ì§€ë¥¼ íŒŒì‹±í•˜ì—¬ íŠ¹ì • ë‚ ì§œ/ë¼ë‹ˆ ë©”ë‰´ë¥¼ ë°˜í™˜.
    ì œí•œ: ì„œë²„ê°€ ì£¼ì°¨ ë³€ê²½ì„ JS/í¼ìœ¼ë¡œ ì²˜ë¦¬í•˜ë©´ ê³¼ê±°/ë¯¸ë˜ ì£¼ ì„ íƒì€ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ. ì´ ê²½ìš° í˜„ì¬ ì£¼ë§Œ ë°˜í™˜.
    """
    t0 = time.time()
    print(f"[CAF][START] date={date} meal={meal}")
    try:
        target_date = _parse_date_input(date)
    except Exception as e:
        print(f"[CAF][ERROR] date-parse {e}")
        return f"âŒ ë‚ ì§œ í•´ì„ ì‹¤íŒ¨: {e}"

    url = "https://www.halla.ac.kr/kr/211/subview.do"
    try:
        net_t = time.time()
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        print(f"[CAF] fetch ok elapsed={time.time()-net_t:.2f}s status={resp.status_code}")
    except Exception as e:
        print(f"[CAF][ERROR] fetch {e}")
        return f"âŒ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}"

    soup = BeautifulSoup(resp.text, "html.parser")

    # ì£¼ê°„ ë²”ìœ„ í…ìŠ¤íŠ¸ ì°¾ê¸° (ì˜ˆ: 2025.08.25 ~ 2025.08.31)
    text = soup.get_text("\n", strip=True)
    m = re.search(r"(\d{4}\.\d{2}\.\d{2})\s*~\s*(\d{4}\.\d{2}\.\d{2})", text)
    week_start = week_end = None
    if m:
        try:
            week_start = datetime.strptime(m.group(1), "%Y.%m.%d").date()
            week_end = datetime.strptime(m.group(2), "%Y.%m.%d").date()
        except Exception:
            pass

    # í˜„ì¬ ì£¼ í™•ì¸ ë° ëŒ€ìƒ ë‚ ì§œê°€ í•´ë‹¹ ì£¼ì— í¬í•¨ë˜ëŠ”ì§€ ì²´í¬
    if week_start and week_end and not (week_start <= target_date <= week_end):
        # ë‹¤ë¥¸ ì£¼ì¼ ê²½ìš°, ì„œë²„ê°€ íŒŒë¼ë¯¸í„° ì—†ì´ í˜„ì¬ ì£¼ë§Œ ì œê³µí•˜ë©´ í•œê³„ ì•ˆë‚´
        info = f"í˜„ì¬ í˜ì´ì§€ëŠ” {week_start}~{week_end} ì£¼ê°„ ì‹ë‹¨ì…ë‹ˆë‹¤."
        return info + " ì›í•˜ëŠ” ë‚ ì§œëŠ” ë‹¤ë¥¸ ì£¼ì…ë‹ˆë‹¤. í˜ì´ì§€ê°€ ì£¼ì°¨ íŒŒë¼ë¯¸í„°ë¥¼ ì œê³µí•˜ì§€ ì•Šì•„ í˜„ì¬ ì£¼ë§Œ ì¡°íšŒ ê°€ëŠ¥í•©ë‹ˆë‹¤: " + url

    # í…Œì´ë¸” íƒìƒ‰: ìš”ì¼ í—¤ë”ì™€ ë¼ë‹ˆ ë¼ë²¨ì´ ìˆëŠ” í‘œë¥¼ ì°¾ì•„ íŒŒì‹±
    tables = soup.find_all("table")

    days = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
    weekday_idx = target_date.weekday()  # 0=ì›”
    target_day_label = days[weekday_idx]

    def clean(txt: str) -> str:
        return re.sub(r"\s+", " ", txt).strip()

    def pick_table_and_parse() -> dict:
        # ë°˜í™˜: {"ì¡°ì‹": str|None, "ì¤‘ì‹": str|None, "ì„ì‹": str|None}
        result = {"ì¡°ì‹": None, "ì¤‘ì‹": None, "ì„ì‹": None}
        for tbl in tables:
            rows = tbl.find_all("tr")
            if not rows:
                continue
            # 1) ìš”ì¼ ì—´ ì¸ë±ìŠ¤ ë§¤í•‘ ì°¾ê¸° (í—¤ë” 1~2í–‰ì„ ì‚´í´ë´„)
            day_col_index = None
            header_candidates = rows[:2] if len(rows) >= 2 else rows[:1]
            for hdr in header_candidates:
                cells = hdr.find_all(["th", "td"])
                for i, c in enumerate(cells):
                    txt = clean(c.get_text())
                    if target_day_label in txt or (txt.endswith("ìš”ì¼") and target_day_label in txt):
                        day_col_index = i
                        break
                if day_col_index is not None:
                    break

            # ì¼ë¶€ í‘œëŠ” ì²« ì—´ì´ 'êµ¬ë¶„', ì´í›„ ì›”~ì¼ì´ë¯€ë¡œ day_col_indexë¥¼ ëª» ì°¾ìœ¼ë©´ ì›”~ì¼ íŒ¨í„´ìœ¼ë¡œ ì¶”ì •
            if day_col_index is None:
                # í—¤ë” í–‰ì—ì„œ ì›”~ì¼ì´ ì—°ì†ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ í™•ì¸
                for hdr in header_candidates:
                    cells = [clean(c.get_text()) for c in hdr.find_all(["th", "td"])]
                    if any(d in "".join(cells) for d in days):
                        # ê¸°ë³¸ì ìœ¼ë¡œ ì²« ì—´ì´ ë¼ë²¨, ì´í›„ ì›”=1, í™”=2 ...ë¡œ ê°€ì •
                        day_col_index = 1 + weekday_idx
                        break

            if day_col_index is None:
                continue

            # 2) ë¼ë‹ˆ ë¼ë²¨ í–‰ì„ ì°¾ì•„ í•´ë‹¹ ìš”ì¼ ì—´ì˜ ì…€ì„ ì¶”ì¶œ
            for tr in rows:
                cells = tr.find_all(["th", "td"])
                if not cells:
                    continue
                label = clean(cells[0].get_text()) if cells else ""
                # ë¼ë‹ˆëª…ì€ ë³€í˜•ë  ìˆ˜ ìˆì–´ ë¶€ë¶„ ì¼ì¹˜ í—ˆìš© (ì˜ˆ: ì¤‘ì‹(11:30~13:30))
                for meal_label in list(result.keys()):
                    if meal_label in label:
                        # ìš”ì¼ ì—´ì´ ë²”ìœ„ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
                        if len(cells) > day_col_index:
                            result[meal_label] = clean(cells[day_col_index].get_text())
            # í•˜ë‚˜ë¼ë„ ìˆ˜ì§‘ë˜ì—ˆìœ¼ë©´ ì´ í…Œì´ë¸”ì„ ì±„íƒ
            if any(v for v in result.values()):
                return result
        return result

    parse_t = time.time()
    found = pick_table_and_parse()
    print(f"[CAF] primary-parse elapsed={time.time()-parse_t:.2f}s result={found}")

    # í´ë°±: í‘œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ë¼ì¸ ê¸°ë°˜ ì¶”ë¡ (ë¶€ì •í™•í•  ìˆ˜ ìˆìŒ)
    if all(v is None for v in found.values()):
        lines = [l for l in text.split("\n") if l]
        # ë§¤ìš° ë‹¨ìˆœ ì¶”ì •: 'ì¤‘ì‹ | ...' ê°™ì€ ë¼ì¸ì´ ìˆìœ¼ë©´ ê·¸ ë‹¤ìŒ í† í°ë“¤ì„ ì‚¬ìš©
        for key in list(found.keys()):
            for ln in lines:
                if key in ln and "|" in ln:
                    # íŒŒì´í”„ êµ¬ë¶„ìœ¼ë¡œ ë¶„í•´ í›„ ìš”ì¼ ì¸ë±ìŠ¤ ì‚¬ìš©
                    parts = [clean(p) for p in ln.split("|")]
                    # parts ì˜ˆ: [ë¼ë²¨, ì¡°ì‹, ì›”, í™”, ìˆ˜, ...] í˜•íƒœì¼ ìˆ˜ ìˆìŒ â†’ ì›”ì´ partsì—ì„œ ì–´ë””ì— ìˆëŠ”ì§€ ë™ì ìœ¼ë¡œ íƒìƒ‰
                    try:
                        # ì›”~ì¼ ì¤‘ target_day_labelì˜ ì²« ë“±ì¥ ìœ„ì¹˜ë¥¼ ì°¾ìŒ
                        day_pos = None
                        for i, token in enumerate(parts):
                            if token.startswith(target_day_label):
                                day_pos = i
                                break
                        if day_pos is None:
                            # ê¸°ë³¸ ì˜¤í”„ì…‹ ê°€ì •: [ë¼ë²¨, ë¼ë‹ˆ, ì›”, í™”, ìˆ˜, ...]
                            base = 2
                            day_pos = base + weekday_idx
                        if len(parts) > day_pos:
                            found[key] = parts[day_pos]
                    except Exception:
                        pass
                        break

    # ê²°ê³¼ êµ¬ì„±
    day_label = target_day_label
    header = f"í•œë¼ëŒ€ í•™ìƒì‹ë‹¹ ì‹ë‹¨ ({target_date} {day_label})"

    if meal in ("ì¡°ì‹", "ì¤‘ì‹", "ì„ì‹"):
        val = found.get(meal)
        if not val:
            out = header + f"\n[{meal}] ì •ë³´ ì—†ìŒ\nì¶”ê°€ ì‚¬í•­: ì›ë¬¸: {url}"
            print(f"[CAF][END] elapsed={time.time()-t0:.2f}s meal-miss")
            return out
        out = header + f"\n[{meal}] {val}\nì¶”ê°€ ì‚¬í•­: ì›ë¬¸: {url}"
        print(f"[CAF][END] elapsed={time.time()-t0:.2f}s meal-hit")
        return out

    # 3ë¼ ëª¨ë‘ ë°˜í™˜
    lines_out = []
    for k in ["ì¡°ì‹", "ì¤‘ì‹", "ì„ì‹"]:
        v = found.get(k)
        lines_out.append(f"[{k}] {v if v else 'ì •ë³´ ì—†ìŒ'}")
    out = header + "\n" + "\n".join(lines_out) + f"\nì¶”ê°€ ì‚¬í•­: ì›ë¬¸: {url}"
    print(f"[CAF][END] elapsed={time.time()-t0:.2f}s all-meals")
    return out

class FunctionCalling:
    def __init__(self, model, available_functions=None, token_counter=None):
        self.model = model
        self.token_counter = token_counter
        default_functions = {
            "search_internet": search_internet,
            "get_halla_cafeteria_menu": get_halla_cafeteria_menu,
        }

        if available_functions:
            default_functions.update(available_functions)

        self.available_functions = default_functions
       
    async def analyze(self, user_message, tools):
        """ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ í•¨ìˆ˜ì™€ íŒë‹¨ ê·¼ê±°ë¥¼ ë°˜í™˜

        Returns:
            dict: {
                "reasoning": str (íŒë‹¨ ê·¼ê±°),
                "output": list (í•¨ìˆ˜ í˜¸ì¶œ ëª©ë¡, ê¸°ì¡´ response.output í˜•ì‹)
            }
        """
        if not user_message or user_message.strip() == "":
            return {
                "reasoning": "ì…ë ¥ì´ ë¹„ì–´ìˆì–´ í•¨ìˆ˜ë¥¼ ì„ íƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "output": []
            }
        
        # 1ë‹¨ê³„: LLMìœ¼ë¡œ í•¨ìˆ˜ ì„ íƒ ì´ìœ  ìƒì„± (structured output)
        reasoning = None
        try:
            from app.ai.chatbot import character
            from app.ai.llm import get_provider
            
            prompt = [
                {"role": "system", "content": character.decide_function},
                {"role": "user", "content": user_message},
            ]
            
            schema = {
                "type": "object",
                "properties": {
                    "reasoning": {"type": "string"},
                    "selected_tools": {
                        "type": "array",
                        "items": {"type": "string"}
                    }
                },
                "required": ["reasoning", "selected_tools"],
                "additionalProperties": False,
            }
            
            provider = get_provider("function_analyze")
            raw, usage = await provider.structured_completion(prompt, schema)
            raw = raw.strip()

            # âœ… API usage ê¸°ë°˜ í† í° ê³„ì‚°
            if self.token_counter and usage:
                self.token_counter.update_from_api_usage(
                    usage=usage,
                    role="function_analyze",
                    model=provider.get_model_name(),
                    category="function"
                )
            
            if raw:
                payload = json.loads(raw)
                reasoning = payload.get("reasoning", "").strip() or None
                selected_tools = payload.get("selected_tools", [])
        except Exception as e:
            reasoning = f"ì¶”ë¡  ìƒì„± ì‹¤íŒ¨ ({e})"
            selected_tools = []  # exception ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •

        # 2ë‹¨ê³„: ê¸°ì¡´ í•¨ìˆ˜ í˜¸ì¶œ ë¶„ì„ (OpenAI API)

        structured_input = [
            {
                "role": "user",
                "content": [
                    {"type": "input_text", "text": user_message}
                ],
            }
        ]
        try:
            response = client.responses.create(
                model=model.o3_mini,
                input=structured_input,
                tools=tools,
                tool_choice="auto",
            )

            if self.token_counter and hasattr(response, 'usage') and response.usage:
                usage_data = {
                    "input_tokens": getattr(response.usage, "input_tokens", 0),
                    "output_tokens": getattr(response.usage, "output_tokens", 0),
                    "total_tokens": getattr(response.usage, "total_tokens", 0),
                    "reasoning_tokens": getattr(response.usage.output_tokens_details, 'reasoning_tokens', 0) if hasattr(response.usage, 'output_tokens_details') else 0,
                }
                self.token_counter.update_from_api_usage(
                    usage=usage_data,
                    role="function_calling",
                    model=model.o3_mini,
                    category="function",
                    replace=False
                )

            return {
                "reasoning": reasoning,
                "selected_tools": selected_tools,  # reasoningì—ì„œ ì„ íƒëœ ë„êµ¬ ëª©ë¡ ì¶”ê°€
                "output": response.output
            }
        except Exception as e:
            return {
                "reasoning": reasoning,
                "selected_tools": selected_tools,
                "output": []
            }
    

    def run(self, analyzed,context):
        ''' analyzed_dict: í•¨ìˆ˜ í˜¸ì¶œ ì •ë³´, context: í˜„ì¬ ë¬¸ë§¥'''
        context.append(analyzed)
        for tool_call in analyzed:
            if tool_call.get("type") != "function_call":
                continue
            function=tool_call["function"]
            func_name=function["name"]
            #ì‹¤ì œ í•¨ìˆ˜ì™€ ì—°ê²°
            func_to_call = self.available_functions[func_name]

            try:

                func_args=json.loads(function["arguments"])#ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜-> ë¬¸ìì—´ì´ jsoní˜•íƒœì…-> ì´ê±¸ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                
                if func_name == "search_internet":
                    # contextëŠ” ì´ë¯¸ run ë©”ì„œë“œì˜ ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ê³  ìˆìŒ
                    func_response = func_to_call(chat_context=context[:], **func_args)
                else:
                    func_response=func_to_call(**func_args)
                context.append({
                    "tool_call_id": tool_call["id"],
                    "role": "tool",
                    "name": func_name, 
                    "content": str(func_response),
                    "parallel_tool_calls": True
                })#ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¬¸ë§¥ì— ì¶”ê°€
  

            except Exception as e:
                print("Error occurred(run):",e)
                return makeup_response("[run ì˜¤ë¥˜ì…ë‹ˆë‹¤]")

        # í•¨ìˆ˜ ì‹¤í–‰ í›„ ìµœì¢… ì‘ë‹µ ìƒì„±
        response = client.responses.create(model=self.model, input=context)

        # âœ… API usage ì¶”ì  (function_calling ì—­í•  - ì¬í˜¸ì¶œ)
        if self.token_counter and hasattr(response, 'usage') and response.usage:
            usage_data = {
                "input_tokens": getattr(response.usage, "input_tokens", 0),
                "output_tokens": getattr(response.usage, "output_tokens", 0),
                "total_tokens": getattr(response.usage, "total_tokens", 0),
                "reasoning_tokens": getattr(response.usage.output_tokens_details, 'reasoning_tokens', 0) if hasattr(response.usage, 'output_tokens_details') else 0,
            }
            self.token_counter.update_from_api_usage(
                usage=usage_data,  # âœ… ìˆ˜ì •: usage_info â†’ usage
                role="function_calling",
                model=self.model,  # âœ… ì¶”ê°€: í•„ìˆ˜ íŒŒë¼ë¯¸í„°
                category="function",
                replace=False
            )

        return response.model_dump()
    
   
    def call_function(self, analyzed_dict):        
        func_name = analyzed_dict["function_call"]["name"]
        func_to_call = self.available_functions[func_name]                
        try:            
            func_args = json.loads(analyzed_dict["function_call"]["arguments"])
            func_response = func_to_call(**func_args)
            return str(func_response)
        except Exception as e:
            print("Error occurred(call_function):",e)
            return makeup_response("[call_function ì˜¤ë¥˜ì…ë‹ˆë‹¤]")
    