import math
import os
import time
import sys
from pathlib import Path
from . import character

# ì‹¤í–‰ ë°©ì‹ê³¼ ë¬´ê´€í•˜ê²Œ íŒ¨í‚¤ì§€ ë£¨íŠ¸ë¥¼ ì¸ì§€ì‹œí‚¤ê¸° ìœ„í•œ ë¶€íŠ¸ìŠ¤íŠ¸ë©
# (ì˜ˆ: `python chatbotDirectory/chatbot.py`ë¡œ ì§ì ‘ ì‹¤í–‰í•˜ëŠ” ê²½ìš° ëŒ€ë¹„)
_ROOT = Path(__file__).resolve().parent.parent
if str(_ROOT) not in sys.path:
    sys.path.insert(0, str(_ROOT)) 
# ìƒëŒ€ ì„í¬íŠ¸ ëŒ€ì‹  ì ˆëŒ€ ì„í¬íŠ¸ ì‚¬ìš©
from .common import model, makeup_response, client
from .functioncalling import FunctionCalling, tools
from ..loding.vector_db_upload import index, get_embedding
from ..loding.mongodbConnect import collection, MONGO_AVAILABLE
from bson import ObjectId, errors
import json

class ChatbotStream:
    def __init__(self, model,system_role,instruction,**kwargs):
        """
        ì´ˆê¸°í™”:
          - context ë¦¬ìŠ¤íŠ¸ ìƒì„± ë° ì‹œìŠ¤í…œ ì—­í•  ì„¤ì •
          - sub_contexts ì„œë¸Œ ëŒ€í™”ë°© ë¬¸ë§¥ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ {í•„ë“œì´ë¦„,ë¬¸ë§¥,ìš”ì•½,ì§ˆë¬¸} êµ¬ì„±
          - current_field = í˜„ì¬ ëŒ€í™”ë°© ì¶”ì  (ê¸°ë³¸ê°’: ë©”ì¸ ëŒ€í™”ë°©
          - openai.api_key ì„¤ì •
          - ì‚¬ìš©í•  ëª¨ë¸ëª… ì €ì¥
          - ì‚¬ìš©ì ì´ë¦„
          - assistant ì´ë¦„
        """
        self.context = [{"role": "system","content": system_role}]
               
        self.current_field = "main"
        
        self.model = model
        self.instruction=instruction

        self.max_token_size = 16 * 1024 #ìµœëŒ€ í† í°ì´ìƒì„ ì“°ë©´ ì˜¤ë¥˜ê°€ë°œìƒ ë”°ë¼ì„œ í† í° ìš©ëŸ‰ê´€ë¦¬ê°€ í•„ìš”.
        self.available_token_rate = 0.9#ìµœëŒ€í† í°ì˜ 90%ë§Œ ì“°ê² ë‹¤.
    
        

        # ë””ë²„ê·¸ í”Œë˜ê·¸ (í™˜ê²½ë³€ìˆ˜ RAG_DEBUGë¡œ ì œì–´: ê¸°ë³¸ í™œì„±í™”)
        self.debug = os.getenv("RAG_DEBUG", "1") not in ("0", "false", "False")

    def _dbg(self, msg: str):
        """ì‘ì€ ë””ë²„ê·¸ í—¬í¼: RAG ê´€ë ¨ ë‚´ë¶€ ìƒíƒœë¥¼ ë³´ê¸° ì‰½ê²Œ ì¶œë ¥."""
        if self.debug:
            print(f"[RAG-DEBUG] {msg}")

    def add_user_message_in_context(self, message: str):
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€:
          - ì‚¬ìš©ìê°€ ì…ë ¥í•œ messageë¥¼ contextì— user ì—­í• ë¡œ ì¶”ê°€
        """
        assistant_message = {
            "role": "user",
            "content": message,
        }
        if self.current_field == "main":
            self.context.append(assistant_message)

    #ì „ì†¡ë¶€
    def _send_request_Stream(self,temp_context=None):
        
        completed_text = ""

        if temp_context is None:
           current_context = self.get_current_context()
           openai_context = self.to_openai_context(current_context)
           stream = client.responses.create(
            model=self.model,
            input=openai_context,  
            top_p=1,
            stream=True,
            
            text={
                "format": {
                    "type": "text"  # ë˜ëŠ” "json_object" ë“± (Structured Output ì‚¬ìš© ì‹œ)
                }
            }
                )
        else:  
           stream = client.responses.create(
            model=self.model,
            input=temp_context,  # user/assistant ì—­í•  í¬í•¨ëœ list êµ¬ì¡°
            top_p=1,
            stream=True,
            text={
                "format": {
                    "type": "text"  # ë˜ëŠ” "json_object" ë“± (Structured Output ì‚¬ìš© ì‹œ)
                }
            }
                )
        
        loading = True  # deltaê°€ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ ë¡œë”© ì¤‘ ìƒíƒœ ìœ ì§€       
        for event in stream:
            #print(f"event: {event}")
            match event.type:
                case "response.created":
                    print("[ğŸ¤– ì‘ë‹µ ìƒì„± ì‹œì‘]")
                    loading = True
                    # ë¡œë”© ì• ë‹ˆë©”ì´ì…˜ìš© ëŒ€ê¸° ì‹œì‘
                    print("â³ GPTê°€ ì‘ë‹µì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤...")
                    
                case "response.output_text.delta":
                    if loading:
                        print("\n[ğŸ’¬ ì‘ë‹µ ì‹œì‘ë¨ â†“]")
                        loading = False
                    # ê¸€ì ë‹¨ìœ„ ì¶œë ¥
                    print(event.delta, end="", flush=True)
                 

                case "response.in_progress":
                    print("[ğŸŒ€ ì‘ë‹µ ìƒì„± ì¤‘...]")

                case "response.output_item.added":
                    if getattr(event.item, "type", None) == "reasoning":
                        print("[ğŸ§  GPTê°€ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤...]")
                    elif getattr(event.item, "type", None) == "message":
                        print("[ğŸ“© ë©”ì‹œì§€ ì•„ì´í…œ ì¶”ê°€ë¨]")
                #ResponseOutputItemDoneEventëŠ” ìš°ë¦¬ê°€ case "response.output_item.done"ì—ì„œ ì¡ì•„ì•¼ í•´
                case "response.output_item.done":
                    item = event.item
                    if item.type == "message" and item.role == "assistant":
                        for part in item.content:
                            if getattr(part, "type", None) == "output_text":
                                completed_text= part.text
                case "response.completed":
                    print("\n")
                    #print(f"\nğŸ“¦ ìµœì¢… ì „ì²´ ì¶œë ¥: \n{completed_text}")
                case "response.failed":
                    print("âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
                case "error":
                    print("âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì—ëŸ¬ ë°œìƒ!")
                case _:
                    
                    print(f"[ğŸ“¬ ê¸°íƒ€ ì´ë²¤íŠ¸ ê°ì§€: {event.type}]")
        return completed_text
  
  
    def send_request_Stream(self):
      self.context[-1]['content']+=self.instruction
      return self._send_request_Stream()
#ì±—ë´‡ì— ë§ê²Œ ë¬¸ë§¥ íŒŒì‹±
    def add_response(self, response):
        response_message = {
            "role" : response['choices'][0]['message']["role"],
            "content" : response['choices'][0]['message']["content"],
            
        }
        self.context.append(response_message)

    def add_response_stream(self, response):
            """
                ì±—ë´‡ ì‘ë‹µì„ í˜„ì¬ ëŒ€í™”ë°©ì˜ ë¬¸ë§¥ì— ì¶”ê°€í•©ë‹ˆë‹¤.
                
                Args:
                    response (str): ì±—ë´‡ì´ ìƒì„±í•œ ì‘ë‹µ í…ìŠ¤íŠ¸.
                """
            assistant_message = {
            "role": "assistant",
            "content": response,
           
        }
            self.context.append(assistant_message)

    def get_response(self, response_text: str):
        """
        ì‘ë‹µë‚´ìš©ë°˜í™˜:
          - ë©”ì‹œì§€ë¥¼ ì½˜ì†”(ë˜ëŠ” UI) ì¶œë ¥ í›„, ê·¸ëŒ€ë¡œ ë°˜í™˜
        """
        print(response_text['choices'][0]['message']['content'])
        return response_text
#ë§ˆì§€ë§‰ ì§€ì¹¨ì œê±°
    def clean_context(self):
        '''
        1.contextë¦¬ìŠ¤íŠ¸ì— ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ë¶€í„° ì²˜ìŒê¹Œì§€ ìˆœíšŒí•œë‹¤
        2."instruction:\n"ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ìì—´ì„ ë‚˜ëˆˆë‹¤..ì²«userì„ ì°¾ìœ¼ë©´ ì•„ë˜ ê³¼ì •ì„ ì§„í–‰í•œë‹¤,
        3.ì²« ë²ˆì§¸ ë¶€ë¶„ [0]ë§Œ ê°€ì ¸ì˜¨ë‹¤. (ì¦‰, "instruction:\n" ì´ì „ì˜ ë¬¸ìì—´ë§Œ ë‚¨ê¸´ë‹¤.)
        4.strip()ì„ ì ìš©í•˜ì—¬ ì•ë’¤ì˜ ê³µë°±ì´ë‚˜ ê°œí–‰ ë¬¸ìë¥¼ ì œê±°í•œë‹¤.
        '''
        for idx in reversed(range(len(self.context))):
            if self.context[idx]['role']=='user':
                self.context[idx]["content"]=self.context[idx]['content'].split('instruction:\n')[0].strip()
                break
#ì§ˆì˜ì‘ë‹µ í† í° ê´€ë¦¬
    def handle_token_limit(self, response):
        # ëˆ„ì  í† í° ìˆ˜ê°€ ì„ê³„ì ì„ ë„˜ì§€ ì•Šë„ë¡ ì œì–´í•œë‹¤.
        try:
            current_usage_rate = response['usage']['total_tokens'] / self.max_token_size
            exceeded_token_rate = current_usage_rate - self.available_token_rate
            if exceeded_token_rate > 0:
                remove_size = math.ceil(len(self.context) / 10)
                self.context = [self.context[0]] + self.context[remove_size+1:]
        except Exception as e:
            print(f"handle_token_limit exception:{e}")
    def to_openai_context(self, context):
        return [{"role":v["role"], "content":v["content"]} for v in context]

    def get_current_context(self):
        """í˜„ì¬ ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜"""
        return self.context

    def is_question_about_regulation(self, question: str) -> bool:
        self._dbg(f"is_question_about_regulation: LLM íŒë‹¨ ì‹œì‘ - q='{question[:60]}...'")

        prompt = self.to_openai_context([
           {
               "role": "system",
               "content": (
                   character.decide_rag
               ),
           },
           {
               "role": "user",
               "content": question,
           },
           ])
        
        try:
           resp = client.responses.create(
               model=model.advanced,  
               input=prompt,
           )
           answer = resp.output_text.strip()
           if answer.strip().lower() not in ["true", "false"]:
              raise ValueError(f"Unexpected LLM answer: '{answer}'")
          

           decision = answer.strip().lower() == "true"
           self._dbg(f"is_question_about_regulation: LLM ê²°ê³¼='{answer}' -> {decision}")
           return decision
        
        except Exception as e:
           self._dbg(f"is_question_about_regulation: LLM íŒë³„ ì‹¤íŒ¨ - {e}")
           # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ
           keywords = ["í•™ì‚¬", "ê·œì •", "ì¡¸ì—…", "ìˆ˜ê°•", "ì„±ì ", "ì¥í•™", "ì§•ê³„"]
           decision = any(k in question for k in keywords)
           self._dbg(f"is_question_about_regulation: fallback ê²°ì • -> {decision}")
           return decision







    def search_similar_chunks(self, query: str, threshold=0.4):
        t0 = time.time()
        self._dbg(f"search_similar_chunks: query='{query[:80]}', threshold={threshold}")
        embedding = get_embedding(query)
        namespaces = ["law_articles", "appendix_tables"]

        all_hits = []
        all_chunk_ids = []

        for ns in namespaces:
            self._dbg(f" - querying namespace='{ns}' top_k=5 include_metadata=True")
            query_response = index.query(
                namespace=ns,
                top_k=5,
                include_metadata=True,
                vector=embedding,
            )
            hits = query_response.matches
            self._dbg(f"   -> {len(hits)} matches returned")

            for h in hits:
                all_hits.append(h)
                meta = getattr(h, "metadata", {}) or {}
                # ë©”íƒ€ë°ì´í„° id í‚¤ í›„ë³´ í™•ëŒ€(mongo_id í¬í•¨)
                id_value = (
                    meta.get("id")
                    or meta.get("mongo_id")
                    or meta.get("ID")
                    or meta.get("default")
                )
                score = getattr(h, "score", None)
                if id_value is not None:
                    all_chunk_ids.append(id_value)
                    self._dbg(f"     match: id={id_value} score={score}")
                else:
                    self._dbg(f"     match: id=<missing> score={score} meta_keys={list(meta.keys())}")

        # ì ìˆ˜ ê¸°ì¤€ í•„í„°ë§
        filtered_hits = [hit for hit in all_hits if getattr(hit, "score", 0) >= threshold]
        t1 = time.time()
        self._dbg(
            f"search_similar_chunks: total_hits={len(all_hits)} filtered={len(filtered_hits)} unique_ids={len(set(all_chunk_ids))} took={(t1-t0):.3f}s"
        )

        return filtered_hits, all_chunk_ids

    def fetch_chunks_from_mongo(self, chunk_ids: list):
        self._dbg(f"fetch_chunks_from_mongo: incoming_ids={len(chunk_ids)} (showing up to 5) -> {chunk_ids[:5]}")
        if not MONGO_AVAILABLE:
            self._dbg("fetch_chunks_from_mongo: Mongo unavailable (ping failed) -> skip and return []")
            return []
        results = []
        for chunk_id in chunk_ids:
            try:
                if isinstance(chunk_id, str) and len(chunk_id) == 24:
                    chunk_id_obj = ObjectId(chunk_id)
                    self._dbg(f" - id '{chunk_id}' converted to ObjectId")
                else:
                    chunk_id_obj = chunk_id
                    self._dbg(f" - id '{chunk_id}' used as-is (type={type(chunk_id).__name__})")
            except errors.InvalidId as e:
                print(f"[WARN] ObjectId ë³€í™˜ ì‹¤íŒ¨: {chunk_id} ({e})")
                chunk_id_obj = chunk_id
            try:
                doc = collection.find_one({"_id": chunk_id_obj})
            except Exception as e:
                self._dbg(f"   -> Mongo query error for _id={chunk_id}: {e}")
                continue
            if doc:
                results.append(doc)
                text_len = len(doc.get("text", "")) if isinstance(doc.get("text"), str) else 0
                self._dbg(f"   -> Mongo HIT _id={doc.get('_id')} text_len={text_len}")
            else:
                self._dbg(f"   -> Mongo MISS _id={chunk_id}")
        self._dbg(f"fetch_chunks_from_mongo: retrieved={len(results)}")
        return results

    def prepare_rag_context(self, user_question: str):
        self._dbg("prepare_rag_context: start")
        if not self.is_question_about_regulation(user_question):
            print("[INFO] í•™ì‚¬ ê·œì • ê´€ë ¨ì´ ì•„ë‹˜ â†’ RAG ê²€ìƒ‰ ì•ˆ í•¨")
            self._dbg("prepare_rag_context: gate=NON_REGULATION -> None")
            return None

        hits, chunk_ids = self.search_similar_chunks(user_question)
        if not hits:
            print("[INFO] Pineconeì—ì„œ ìœ ì‚¬ ë°ì´í„° ì—†ìŒ")
            self._dbg("prepare_rag_context: no pinecone hits -> None")
            return None

        self._dbg(f"prepare_rag_context: chunk_ids(sample)={chunk_ids[:5]} total={len(chunk_ids)}")

        if not chunk_ids:
            print("[INFO] Pinecone ê²°ê³¼ì— id ì—†ìŒ")
            self._dbg("prepare_rag_context: ids empty -> None")
            return None

        chunks = self.fetch_chunks_from_mongo(chunk_ids)
        if not chunks:
            print("[INFO] MongoDBì—ì„œ ë§¤ì¹­ëœ ë¬¸ì„œ ì—†ìŒ")
            # í´ë°±: Pinecone ë©”íƒ€ë°ì´í„°ì˜ text_previewë¡œ ìµœì†Œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            previews = []
            for h in hits:
                meta = getattr(h, "metadata", {}) or {}
                tp = meta.get("text_preview")
                if isinstance(tp, str) and tp.strip():
                    previews.append(tp.strip())
            if previews:
                rag_ctx = "\n\n".join(previews)
                self._dbg(f"prepare_rag_context: fallback to previews chars={len(rag_ctx)} count={len(previews)}")
                return rag_ctx
            self._dbg("prepare_rag_context: mongo returned 0 and no previews -> None")
            return None

        texts = [chunk.get("text", "") for chunk in chunks]
        rag_ctx = "\n\n".join(texts)
        self._dbg(f"prepare_rag_context: built context chars={len(rag_ctx)}")
        return rag_ctx
    
    def get_rag_context(self, user_question: str):
        """RAG ì»¨í…ìŠ¤íŠ¸ë§Œ ì¤€ë¹„í•˜ì—¬ ë°˜í™˜ (ì—†ìœ¼ë©´ None). ëª¨ë¸ í˜¸ì¶œì€ í•˜ì§€ ì•ŠìŒ."""
        return self.prepare_rag_context(user_question)
    
        
    def get_response_from_db_only(self, user_question: str):
        self._dbg("get_response_from_db_only: start")
        rag_context = self.prepare_rag_context(user_question)
        if rag_context is None:
            self._dbg("ê¸°ì–µê²€ìƒ‰ ì•„ë‹˜")
            return False

        # LLM í˜¸ì¶œí•  context êµ¬ì„±: system ë©”ì‹œì§€ + DB ë‚´ìš©(system role) + user ì§ˆë¬¸
        context = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ í•™ì‚¬ ê·œì • ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."},
            {"role": "system", "content": rag_context},
            {"role": "user", "content": user_question},
        ]
        self._dbg(
            f"get_response_from_db_only: messages=[system, system(ctx:{len(rag_context)} chars), user] model={self.model}"
        )

        return self._send_request_Stream(temp_context=self.to_openai_context(context))
 


if __name__ == "__main__":
    '''ì‹¤í–‰íë¦„
    ë‹¨ê³„	ë‚´ìš©
1ï¸âƒ£	ì‚¬ìš©ì ì…ë ¥ ë°›ìŒ (user_input)
2ï¸âƒ£	â†’ add_user_message_in_context() ë¡œ user ë©”ì‹œì§€ë¥¼ ë¬¸ë§¥ì— ì¶”ê°€
3ï¸âƒ£	â†’ analyze() ë¡œ í•¨ìˆ˜ í˜¸ì¶œì´ í•„ìš”í•œì§€ íŒë‹¨
4ï¸âƒ£	â†’ í•„ìš”í•˜ë©´ í•¨ìˆ˜ ì‹¤í–‰ + ê²°ê³¼ë¥¼ temp_contextì— ì¶”ê°€
5ï¸âƒ£	â†’ chatbot._send_request_Stream(temp_context) ë¡œ ì‘ë‹µ ë°›ìŒ
6ï¸âƒ£	âœ… streamed_response ê²°ê³¼ë¥¼ ì§ì ‘ add_response_stream()ìœ¼ë¡œ ìˆ˜ë™ ì €ì¥'''
    system_role = "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ ì±—ë´‡ì…ë‹ˆë‹¤."
    instruction = "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•˜ê³ , í•„ìš”í•œ ê²½ìš° í•¨ìˆ˜ í˜¸ì¶œì„ í†µí•´ ì¶”ê°€ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
    # ChatbotStream ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    chatbot = ChatbotStream(
        model.advanced,
        system_role=system_role,
        instruction=instruction,
        user="ëŒ€ê¸°",
        assistant="memmo")
    func_calling=FunctionCalling(model.advanced)
    print("===== Chatbot Started =====")
    print("ì´ˆê¸° context:", chatbot.context)
    print("ì‚¬ìš©ìê°€ 'exit'ë¼ê³  ì…ë ¥í•˜ë©´ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")
    
   # ì¶œë ¥: {}
    

    while True:
        try:
            user_input = input("User > ")

            if user_input.strip().lower() == "exit":
                print("Chatbot ì¢…ë£Œ.")
                break

            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            chatbot.add_user_message_in_context(user_input)

            # 1) ê¸°ì–µê²€ìƒ‰ í›„ë³´ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            rag_ctx = chatbot.get_rag_context(user_input)
            has_rag = rag_ctx is not None and len(rag_ctx.strip()) > 0

            # 2) í•¨ìˆ˜ í˜¸ì¶œ ë¶„ì„ ë° ì‹¤í–‰
            analyzed = func_calling.analyze(user_input, tools)
            func_msgs = []  # function_call + function_call_output ë©”ì‹œì§€ ëˆ„ì 
            func_outputs = []  # í•¨ìˆ˜ ê²°ê³¼ ë¬¸ìì—´ ëˆ„ì 

            for tool_call in analyzed:  # analyzedëŠ” list of function_call objects
                if getattr(tool_call, "type", None) != "function_call":
                    continue

                func_name = tool_call.name
                func_args = json.loads(tool_call.arguments)
                call_id = tool_call.call_id

                func_to_call = func_calling.available_functions.get(func_name)
                if not func_to_call:
                    print(f"[ì˜¤ë¥˜] ë“±ë¡ë˜ì§€ ì•Šì€ í•¨ìˆ˜: {func_name}")
                    continue

                try:
                    # ì•ˆì „ ê¸°ë³¸ê°’ ë³´ê°•: ë¶„ì„ê¸°ê°€ ì¼ë¶€ ì¸ìë¥¼ ìƒëµí•´ë„ ë™ì‘í•˜ë„ë¡
                    if func_name == "get_halla_cafeteria_menu":
                        func_args.setdefault("date", "ì˜¤ëŠ˜")
                        func_args.setdefault("meal", "ì¤‘ì‹")
                    func_response = (
                        func_to_call(chat_context=chatbot.context[:], **func_args)
                        if func_name == "search_internet"
                        else func_to_call(**func_args)
                    )

                    # function_call/ output ë©”ì‹œì§€ êµ¬ì„±
                    func_msgs.extend([
                        {
                            "type": "function_call",
                            "call_id": call_id,
                            "name": func_name,
                            "arguments": tool_call.arguments,
                        },
                        {
                            "type": "function_call_output",
                            "call_id": call_id,
                            "output": str(func_response),
                        },
                    ])
                    func_outputs.append(str(func_response))
                except Exception as e:
                    print(f"[í•¨ìˆ˜ ì‹¤í–‰ ì˜¤ë¥˜] {func_name}: {e}")

            has_funcs = len(func_outputs) > 0

            # ë³´ê°•: í•™ì‹/ì‹ë‹¨ ì§ˆì˜ì¼ ê²½ìš°, ë¶„ì„ê¸°ê°€ í˜¸ì¶œì„ ì•ˆ í–ˆë”ë¼ë„ ì§ì ‘ í•¨ìˆ˜ í˜¸ì¶œ ì‹œë„
            lowered = user_input.lower()
            if ("í•™ì‹" in lowered) or ("ì‹ë‹¨" in lowered) or ("ì ì‹¬" in lowered) or ("ì €ë…" in lowered) or ("ë©”ë‰´" in lowered) or ("ì¡°ì‹" in lowered):
                if not has_funcs:
                    try:
                        # ê¸°ë³¸ê°’: ì˜¤ëŠ˜/ì¤‘ì‹, ê°„ë‹¨ ê·œì¹™ìœ¼ë¡œ ë¼ë‹ˆ/ë‚ ì§œ ì¶”ì¶œ
                        meal_pref = "ì¤‘ì‹"
                        if ("ì¡°ì‹" in lowered) or ("ì•„ì¹¨" in lowered):
                            meal_pref = "ì¡°ì‹"
                        elif ("ì„ì‹" in lowered) or ("ì €ë…" in lowered):
                            meal_pref = "ì„ì‹"
                        # ë‚ ì§œ í‚¤ì›Œë“œ
                        date_pref = "ì˜¤ëŠ˜"
                        if "ë‚´ì¼" in lowered:
                            date_pref = "ë‚´ì¼"
                        else:
                            import re as _re
                            m = _re.search(r"(\d{4}[./-]\d{1,2}[./-]\d{1,2})", user_input)
                            if m:
                                date_pref = m.group(1)
                        caf_args = {"date": date_pref, "meal": meal_pref}
                        from chatbotDirectory.functioncalling import get_halla_cafeteria_menu
                        caf_out = get_halla_cafeteria_menu(**caf_args)
                        # ë©”ì‹œì§€ í˜•íƒœë¡œ ì‚½ì…í•˜ì—¬ ëª¨ë¸ì´ ê·¼ê±°ë¡œ í™œìš©
                        call_id = "cafeteria_auto"
                        func_msgs.extend([
                            {
                                "type": "function_call",
                                "call_id": call_id,
                                "name": "get_halla_cafeteria_menu",
                                "arguments": json.dumps(caf_args, ensure_ascii=False),
                            },
                            {
                                "type": "function_call_output",
                                "call_id": call_id,
                                "output": str(caf_out),
                            },
                        ])
                        func_outputs.append(str(caf_out))
                        has_funcs = True
                    except Exception as e:
                        print(f"[ë³´ê°• í˜¸ì¶œ ì‹¤íŒ¨] get_halla_cafeteria_menu: {e}")

            # 3) ìµœì¢… temp_context êµ¬ì„±
            temp_context = chatbot.to_openai_context(chatbot.context[:])

            # ì „ë°˜ ì§€ì¹¨: ì‚¬ìš©ì ì¿¼ë¦¬ì™€ í†µí•© ì§€ì‹œ
            temp_context.append({
                "role": "system",
                "content": (
                    f"ì´ê²ƒì€ ì‚¬ìš©ì ì¿¼ë¦¬ì…ë‹ˆë‹¤: {user_input}\n"
                    "ë‹¤ìŒ ì •ë³´ë¥¼ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ëŒ€ë‹µì— ë§ê²Œ í†µí•©í•´ ì „ë‹¬í•˜ì„¸ìš”.\n"
                    "- í•¨ìˆ˜í˜¸ì¶œ ê²°ê³¼: ìˆìœ¼ë©´ ë°˜ì˜\n- ê¸°ì–µê²€ìƒ‰ ê²°ê³¼: ìˆìœ¼ë©´ ë°˜ì˜"
                ),
            })
            # ì¼ë°˜ ì§€ì¹¨ ì¶”ê°€
            temp_context.append({"role": "system", "content": chatbot.instruction})

            if has_rag:
                # RAG ì•ˆë‚´ + ê·¼ê±° íˆ¬ì…
                temp_context.append({"role": "system", "content": "ê²€ìƒ‰ê²°ê³¼ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì›í•˜ëŠ” ì¿¼ë¦¬ì— ë§ê²Œ ëŒ€ë‹µí•˜ì„¸ìš”."})
                temp_context.append({"role": "system", "content": f"[ê²€ìƒ‰ê²°ê³¼]\n{rag_ctx}"})

            if has_funcs:
                # í•¨ìˆ˜ í˜¸ì¶œ ê²°ê³¼ ì•ˆë‚´ ë° ë©”ì‹œì§€ ì‚½ì…
                temp_context.append({"role": "system", "content": "í•¨ìˆ˜í˜¸ì¶œê²°ê³¼ì…ë‹ˆë‹¤. ì´ê±¸ ë°”íƒ•ìœ¼ë¡œ ëŒ€ë‹µì— ì‘í•˜ì„¸ìš”."})
                temp_context.extend(func_msgs)

            if has_rag and has_funcs:
                temp_context.append({
                    "role": "system",
                    "content": "ì•„ë˜ í•¨ìˆ˜ í˜¸ì¶œ ê²°ê³¼ì™€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ëª¨ë‘ í™œìš©í•´, ë‘ ë¬¸ë§¥ì´ ì–´ë–»ê²Œ ë„ì¶œë˜ì—ˆëŠ”ì§€ í•œ ì¤„ë¡œ ì„¤ëª…í•˜ê³  ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.",
                })

            if not has_rag and not has_funcs:
                # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ì¼ë°˜ ì±—ë´‡ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì‘ë‹µ
                print("RAG/í•¨ìˆ˜í˜¸ì¶œ ê²°ê³¼ ì—†ìŒ â†’ ì¼ë°˜ ì±—ë´‡ ì‘ë‹µ")
                streamed = chatbot.send_request_Stream()
                chatbot.add_response_stream(streamed)
                print("\n===== Chatbot Context Updated =====")
                print(chatbot.context)
                continue

            # 4) ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­
            streamed_response = chatbot._send_request_Stream(temp_context=temp_context)
            chatbot.add_response_stream(streamed_response)

            print("\n===== Chatbot Context Updated =====")
            print(chatbot.context)

        except KeyboardInterrupt:
            print("\nì‚¬ìš©ì ì¢…ë£Œ(Ctrl+C)")
            break
        except Exception as e:
            print(f"[ë£¨í”„ ì—ëŸ¬] {e}")
            continue